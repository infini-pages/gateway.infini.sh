'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/references/filters/echo/','title':"echo",'section':"Online Filter",'content':"echo #  Description #  The echo filter is used to output specified characters in the returned result. It is often used for debugging.\nFunction Demonstration #    Configuration Example #  A simple example is as follows:\nflow: - name: hello_world filter: - echo: message: \u0026quot;hello infini\\n\u0026quot; The echo filter allows you to set the number of times that same characters can be output repeatedly. See the following example.\n... - echo: message: \u0026quot;hello gateway\\n\u0026quot; repeat: 3 ... Parameter Description #     Name Type Description     message string Characters to be output   repeat int Number of repetition times   stdout bool Whether the terminal also outputs the characters. The default value is false.    "});index.add({'id':1,'href':'/docs/overview/','title':"Overview",'section':"Docs",'content':"Overview #  Introduction #  INFINI Gateway is a high performance gateway for Elasticsearch. It offers a broad range of features and is easy to use. INFINI Gateway works in the same way as a common reverse proxy. It is usually deployed in front of Elasticsearch clusters. All requests are sent to the gateway instead of Elasticsearch, and then the gateway forwards the requests to the back-end Elasticsearch clusters. The gateway is deployed between the client and Elasticsearch. Therefore, the gateway can be configured to perform index-level traffic control and throttling, cache acceleration for common queries, query request audit, and dynamic modification of query results.\nFeatures #  INFINI Gateway caters to Elasticsearch well. Many Elasticsearch-related service scenarios and characteristics are taken into account in the design. Therefore, INFINI Gateway is tailored to provide many useful features for Elasticsearch.\nLightweight INFINI Gateway is written in Golang. The installation package is only about 10 MB and has no external environment dependency. Deployment and installation are very simple. Users can simply download the binary executable file of the gateway program from the platform and execute the program file.  Optimal Performance INFINI Gateway is designed to run in an optimized state during programming. Test results show that INFINI Gateway provides a speed that is over 25% faster than mainstream gateway counterparts, and which has been optimized to allow Elasticsearch to double the write and query speeds.   Cross-Version Support INFINI Gateway is compatible with different Elasticsearch versions to ensure seamless adaptation of the service code. The back-end Elasticsearch cluster versions can be upgraded seamlessly, which reduces the complexity of version upgrade and data migration.  Observability INFINI Gateway can dynamically intercept and analyze requests generated during the running of Elasticsearch. Users can learn about the running status of the entire cluster from indicators and logs, in an effort to improve performance and optimize services. It can be also used for auditing and slow query analysis.   High Availability INFINI Gateway has multiple built-in high availability (HA) solutions. The front-end request entry supports virtual IP-based dual-node hot standby. The back-end cluster supports auto perception of the cluster topology, auto discovery of nodes that go online/offline, auto processing of back-end faults, and auto retry and migration of requests.  Flexible and Extensible Each module of INFINI Gateway can be independently extended and each request can be flexibly handled and routed. INFINI Gateway supports intelligent learning of routes and provides rich internal filters. The processing logic of each request can be dynamically modified. INFINI Gateway can also be extended using plug-ins.   Seamless Integration #  The external interfaces provided by INFINI Gateway are fully compatible with Elasticsearch\u0026rsquo;s native interfaces. The integration is very simple and can be completed by changing the configuration pointed to Elasticsearch to the address of the gateway.\nWhy Is INFINI Gateway Needed? #  I largely understand the above integration interaction diagram and am familiar with the use of Elasticsearch. Why do I need to place a gateway in front of it?\nIf the scale of your Elasticsearch cluster is quite large, consider the following scenarios:\nWAF and Security #  The prevalence of Elasticsearch makes it a prime target for hackers, which necessitates the use of a Web application firewall (WAF). Whether it\u0026rsquo;s the use of cross-site script attacks, cross-site scripting injection, weak passwords, brute force cracking, or unreasonable query parameter abuse by programmers, INFINI Gateway is able to detect and verify requests from different Web application clients. It utilizes a series of Elasticsearch security policies to ensure security and legitimacy and block illegitimate requests in real time.\nCluster Upgrade #  The Elasticsearch iteration is pretty fast and cluster upgrade needs to be handled frequently. However, the following points must be considered in the cluster upgrade:\n Minimal downtime: The service data writing and query cannot be interrupted due to the cluster upgrade. Data can be continuously written but data cannot be lost due to the restart of back-end nodes. Cluster traffic switching. You need to determine when and how to switch traffic from the old cluster to the new one, whether to modify the service code or configuration file, how to roll back and restore the system, and whether to release a new deployment package.  With INFINI Gateway, you do not need to care about the back-end Elasticsearch clusters in the service code but only need to access the fixed address of the gateway. Then, INFINI Gateway will solve all problems for you.\nIndex Rebuilding #  Index rebuilding is required when mappings or the segmentation dictionary is modified. Data writing cannot be stopped during rebuilding, data must be consistent after rebuilding, and new data and modified data must be handled, which are cumbersome. INFINI Gateway supports one-click index rebuilding and automatically records any document modifications that take place during rebuilding. It switches from the old index to the new one seamlessly after rebuilding, which is completely imperceptive to front-end applications.\nThrottling and Traffic Control #  A cluster may break down due to burst traffic or become overburdened due to large indexes. For this, you need to manage abnormal traffic in order to protect the entire Elasticsearch cluster against abnormal traffic and even malicious attacks. INFINI Gateway can control the traffic flexibly and allows setting index-level traffic control rules. There are a thousand traffic control rules for a thousand indexes.\nSlow Query #  The built-in cache function of INFINI Gateway can cache the most common queries and warm up specific queries according to periodic query plans to ensure that queries are hit each time for front-end services, thereby increasing query speed and improving user\u0026rsquo;s service query experience.\nSlow Indexing #  INFINI Gateway can combine many small batches of Elasticsearch index requests from different clients into one large bulk request, and deliver the index requests to a specified node of a specified shard through shard-level precision routing. In this way, back-end Elasticsearch does not need to forward the requests, saving Elasticsearch resources and bandwidth, and improving overall throughput and performance of the cluster.\nRequest Mutation #  What happens if you discover errors in query statements after the code goes live? With INFINI gateway, you do not need to worry about it. You can rewrite a specified query of a specified service online and correct the query statements dynamically, without re-publishing the application, which offers convenience and flexibility. If you are not satisfied with the JSON query results returned by Elasticsearch, you can utilize INFINI gateway to dynamically replace query results, and even merge data from other sources such as Hbase and MySQL into the required JSON data, which is then returned to the client.\nRequest Analysis #  People complain about the slow response of Elasticsearch, but do you know which indexes in Elasticsearch are slow? Which queries cause slow response? Which users are accountable for the slow response? INFINI Gateway tracks Elasticsearch from clusters to indexes, from indexes to queries, and from applications to users so that you know every detail about the Elasticsearch clusters.\nIn a word, using Elasticsearch together with INFINI gateway will give you a splendid experience.\nArchitecture #  The architecture diagram below shows the core modules of INFINI Gateway.\nThe modules that carry external requests as a proxy are as follows: entry, router, flow, and filter. One entry needs one router, one router can route requests to multiple flows, and one flow is composed of multiple filters.\nEntry #  The entry module defines the request entry for the gateway. INFINI Gateway supports the Hypertext Transfer Protocol (HTTP) and Hypertext Transfer Protocol Secure (HTTPS) modes. It automatically generates certification files in HTTPS mode.\nRouter #  The router module mainly defines routing rules for requests, and routes requests to a specified flow according to the method and request address.\nFlow #  The flow module mainly defines the processing logic of data. Each request will go through a series of filter operations, and a flow is used to organize these filter operations.\nFilter #  The filter module is composed of several different filter components. Each filter is designed to cope with only one task, and multiple filters compose a single flow.\nPipeline #  The pipeline module is composed of several different processor components. Compared with a flow, a pipeline focuses on the processing of offline tasks.\nQueue #  The queue module is an abstract message queue, such as local disk-based reliability message persistence, Redis, Kafka, and other adapters. Different back-end adapters can be set for queues based on scenarios.\nAt the bottom layer of the framework used by INFINI Gateway, there are some common modules, such as the API used to provide an external programming entry and the Elastic module used to handle the API encapsulation for different versions of Elasticsearch.\nNext #   View Downloading and Installation  "});index.add({'id':2,'href':'/docs/tutorial/log4j2_filtering/','title':"Apache Log4j Vulnerability Processing",'section':"Tutorials",'content':"Apache Log4j Vulnerability Processing #  CVE Address\n https://github.com/advisories/GHSA-jfh8-c2jp-5v3q\nVulnerability Description\nApache Log4j is a very popular open source logging toolkit used for the Java runtime environment. Many Java frameworks including Elasticsearch of the latest version, use this component. Therefore, the scope of impact is huge.\nThe latest vulnerability existing in the execution of Apache Log4j\u0026rsquo;s remote code was revealed recently. Attackers can construct malicious requests and utilize this vulnerability to execute arbitrary code on a target server. As a result, the server can be controlled by hackers, who can then conduct page tampering, data theft, mining, extortion, and other behaviors. Users who use this component are advised to immediately initiate emergency response for fixing.\nBasically, if a log output by Log4j contains the keyword ${, the log is replaced as a variable and then the variable operation is executed. Attackers can maliciously construct log content to make Java processes to execute arbitrary commands, achieving the attack purpose.\nVulnerability Level: very urgent\nThe vulnerability is caused by the lookup function provided by Log4j2. This function allows developers to read configurations in the environment by using a number of protocols. However, the input is not strictly judged in the implementation, resulting in the vulnerability.\nImpact Scope: Java products: Apache Log4j 2.x \u0026lt; 2.15.0-rc2\nAttack Detection\nYou can check logs for jndi:ldap://, jndi:rmi, and other characters to find out possible attacks.\nHandling Method #  If Elasticsearch does not support configuration modification, Jar package replacement of Log4j, or cluster restart, you can use INFINI Gateway to intercept requests, replace parameters, and even directly block requests. You can use INFINI Gateway to check parameters in requests sent to Elasticsearch and replace or reject content that contains the sensitive keyword ${. In this way, INFINI Gateway can prevent the execution of malicious attack commands during Log4j logging after attack-contained requests are sent to Elasticsearch, thereby preventing attacks.\nReference Configuration #  Download the latest 1.5.0-SNAPSHOT version: http://release.elasticsearch.cn/gateway/snapshot/\nThe context_filter filter of INFINI Gateway can be used to detect the keywords of the request context _ctx.request.to_string and filter out malicious traffic, thereby blocking attacks.\npath.data: data path.logs: log entry: - name: es_entrypoint enabled: true router: default max_concurrency: 20000 network: binding: 0.0.0.0:8000 router: - name: default default_flow: main_flow flow: - name: main_flow filter: - context_filter: context: _ctx.request.to_string action: redirect_flow status: 403 flow: log4j_matched_flow must_not: # any match will be filtered regex: - \\$\\{.*?\\} - \u0026quot;%24%7B.*?%7D\u0026quot; #urlencode contain: - \u0026quot;jndi:\u0026quot; - \u0026quot;jndi:ldap:\u0026quot; - \u0026quot;jndi:rmi:\u0026quot; - \u0026quot;jndi%3A\u0026quot; #urlencode - \u0026quot;jndi%3Aldap%3A\u0026quot; #urlencode - \u0026quot;jndi%3Armi%3A\u0026quot; #urlencode - elasticsearch: elasticsearch: es-server - name: log4j_matched_flow filter: - echo: message: 'Apache Log4j 2, Boom!' elasticsearch: - name: es-server enabled: true endpoints: - http://localhost:9200 Use urlencode to convert the test command ${java:os} into %24%7Bjava%3Aos%7D.\nRequest calling execution result when requests do not need to pass through the gateway:\n~% curl 'http://localhost:9200/index1/_search?q=%24%7Bjava%3Aos%7D' {\u0026quot;error\u0026quot;:{\u0026quot;root_cause\u0026quot;:[{\u0026quot;type\u0026quot;:\u0026quot;index_not_found_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;no such index\u0026quot;,\u0026quot;resource.type\u0026quot;:\u0026quot;index_or_alias\u0026quot;,\u0026quot;resource.id\u0026quot;:\u0026quot;index1\u0026quot;,\u0026quot;index_uuid\u0026quot;:\u0026quot;_na_\u0026quot;,\u0026quot;index\u0026quot;:\u0026quot;index1\u0026quot;}],\u0026quot;type\u0026quot;:\u0026quot;index_not_found_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;no such index\u0026quot;,\u0026quot;resource.type\u0026quot;:\u0026quot;index_or_alias\u0026quot;,\u0026quot;resource.id\u0026quot;:\u0026quot;index1\u0026quot;,\u0026quot;index_uuid\u0026quot;:\u0026quot;_na_\u0026quot;,\u0026quot;index\u0026quot;:\u0026quot;index1\u0026quot;},\u0026quot;status\u0026quot;:404}% Logs on Elasticsearch are as follows:\n[2021-12-11T01:49:50,303][DEBUG][r.suppressed ] path: /index1/_search, params: {q=Mac OS X 10.13.4 unknown, architecture: x86_64-64, index=index1} org.elasticsearch.index.IndexNotFoundException: no such index at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.infe(IndexNameExpressionResolver.java:678) ~[elasticsearch-5.6.15.jar:5.6.15] at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.innerResolve(IndexNameExpressionResolver.java:632) ~[elasticsearch-5.6.15.jar:5.6.15] at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.resolve(IndexNameExpressionResolver.java:580) ~[elasticsearch-5.6.15.jar:5.6.15] The logs above show that q=${java:os} in query conditions is executed and is changed to q=Mac OS X 10.13.4 unknown, architecture: x86_64-64, index=index1.\nRequest calling execution result when requests need to pass through the gateway:\nmedcl@Medcl:~% curl 'http://localhost:8000/index1/_search?q=%24%7Bjava%3Aos%7D' Apache Log4j 2, Boom!% The logs above show that requests are filtered out.\nYou can try other commands to check whether malicious requests are intercepted:\n#{java:vm} ~% curl 'http://localhost:9200/index/_search?q=%24%7Bjava%3Avm%7D' [2021-12-11T02:36:04,764][DEBUG][r.suppressed ] [Medcl-2.local] path: /index/_search, params: {q=OpenJDK 64-Bit Server VM (build 25.72-b15, mixed mode), index=index} ~% curl 'http://localhost:8000/index/_search?q=%24%7Bjava%3Avm%7D' Apache Log4j 2, Boom!% #{jndi:rmi://localhost:1099/api} ~% curl 'http://localhost:9200/index/_search?q=%24%7Bjndi%3Armi%3A%2F%2Flocalhost%3A1099%2Fapi%7D' 2021-12-11 03:35:06,493 elasticsearch[YOmFJsW][search][T#3] ERROR An exception occurred processing Appender console java.lang.SecurityException: attempt to add a Permission to a readonly Permissions object ~% curl 'http://localhost:8000/index/_search?q=%24%7Bjndi%3Armi%3A%2F%2Flocalhost%3A1099%2Fapi%7D' Apache Log4j 2, Boom!%  The benefits of using INFINI Gateway is that no change needs to be made to the Elasticsearch server, especially in large-scale cluster scenarios. The flexible INFINI Gateway can significantly reduce workload, improve efficiency, shorten the security processing time, and reduce enterprise risks.\n "});index.add({'id':3,'href':'/docs/tutorial/online_query_rewrite/','title':"Online Query Repair",'section':"Tutorials",'content':"Online Query Repair #  In some cases, you may find that the QueryDSL generated by the service code is unreasonable. The general practice is to modify the service code and publish it online. If the launch of a new version takes a long time (for example, the put-into-production window is not reached, major network operation closure is in progress, or additional code needs to be submitted to go live), a large number of tests need to be performed. However, faults in the production environment need to be rectified immediately and customers have no time to wait. What should be done in that case?\nDon\u0026rsquo;t worry. You can use INFINI Gateway to dynamically repair queries.\nExample #  See the following query example:\nGET _search { \u0026quot;size\u0026quot;: 1000000 , \u0026quot;explain\u0026quot;: true } The size parameter is set to a very large value and the problem is not found at the beginning. With more and more data generated, too much returned data is bound to cause a sharp decline in performance. In addition, enabling the explain parameter will create unnecessary performance overhead and this function is generally used only during development and debugging.\nBy adding the request_body_json_set filter to the gateway, you can dynamically replace the value of the specified request body JSON PATH. The configuration for the above example is as follows:\nflow: - name: rewrite_query filter: - request_body_json_set: path: - explain -\u0026gt; false - size -\u0026gt; 10 - dump_request_body: - elasticsearch: elasticsearch: dev Set the explain and size parameters again. The query is rewritten in the following format before it is sent to Elasticsearch:\n{ \u0026quot;size\u0026quot;: 10, \u0026quot;explain\u0026quot;: false } The problem is successfully fixed in in-service mode.\nAnother Example #  Look at the following query example. The programmer who writes the code writes the name of the field to be queried by mistake. The name should be name but is written as name1. The size parameter is set to a very large value.\nGET medcl/_search { \u0026quot;aggs\u0026quot;: { \u0026quot;total_num\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;name1\u0026quot;, \u0026quot;size\u0026quot;: 1000000 } } } } The system goes live but a problem arises when a query is conducted. For this problem, you can add the following filter configuration to the gateway request flow:\nflow: - name: rewrite_query filter: - request_body_json_set: path: - aggs.total_num.terms.field -\u0026gt; \u0026quot;name\u0026quot; - aggs.total_num.terms.size -\u0026gt; 10 - size -\u0026gt; 0 - dump_request_body: - elasticsearch: elasticsearch: dev In the above configuration, we can replace the data of the JSON request body through its path, and add one parameter not to return the query document because only aggregated results are required.\nAnother Example #  The user query is as follows:\n{ \u0026quot;query\u0026quot;:{ \u0026quot;bool\u0026quot;:{ \u0026quot;should\u0026quot;:[{\u0026quot;term\u0026quot;:{\u0026quot;isDel\u0026quot;:0}},{\u0026quot;match\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;order\u0026quot;}}] }\t} } Now you want to replace the term query with the equivalent range query as follows:\n{ \u0026quot;query\u0026quot;:{ \u0026quot;bool\u0026quot;:{ \u0026quot;should\u0026quot;:[{ \u0026quot;range\u0026quot;: { \u0026quot;isDel\u0026quot;: {\u0026quot;gte\u0026quot;: 0,\u0026quot;lte\u0026quot;: 0 }}},{\u0026quot;match\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;order\u0026quot;}}] }\t} } Use the following configuration:\nflow: - name: rewrite_query filter: - request_body_json_del: path: - query.bool.should.[0] - request_body_json_set: path: - query.bool.should.[1].range.isDel.gte -\u0026gt; 0 - query.bool.should.[1].range.isDel.lte -\u0026gt; 0 - dump_request_body: - elasticsearch: elasticsearch: dev In the above configuration, one request_body_json_del filter is used to delete the first element from the Should query, that is, the Term subquery to be replaced. There is only one Match query left. One Should subquery is added, and the added subscript should be 1. Set the attributes of the Range query.\nFurther Improvement #  In the above examples, queries are directly replaced. In general, you may need to make a judgment about whether to replace the query, for example, replacement may only be performed when the _ctx.request.body_json.query.bool.should.[0].term.isDel JSON field exists. The conditional judgment of the gateway is very flexible and the configuration is as follows:\nflow: - name: cache_first filter: - if: and: - has_fields: ['_ctx.request.body_json.query.bool.should.[0].term.isDel'] then: - request_body_json_del: path: - query.bool.should.[0] - request_body_json_set: path: - query.bool.should.[1].range.isDel.gte -\u0026gt; 0 - query.bool.should.[1].range.isDel.lte -\u0026gt; 0 - dump_request_body: - elasticsearch: elasticsearch: dev The feature is superb!\n"});index.add({'id':4,'href':'/docs/references/modules/floating_ip/','title':"Floating IP",'section':"Functional Component",'content':"Floating IP #  The embedded floating IP feature of INFINI Gateway can implement dual-node hot standby and failover. INFINI Gateway innately provides high availability for L4 network traffic, and no extra software and devices are required to prevent proxy service interruption caused by downtime or network failures.\nNote:\n This feature supports only Mac OS and Linux OS. The gateway must run as the user root. This feature relies on the ping and ifconfig commands of the target system. Therefore, ensure that related packages are installed by default. The network interface cards (NICs) of a group of gateways, on which floating IP is enabled, should belong to the same subnet, and devices on the Intranet can communicate with each other in broadcast mode (the actual IP address and floating IP address of a gateway need to be different only in the last bit, for example, 192.168.3.x).   Function Demonstration #    Youtube  Bilibili  What Is a Floating IP? #  INFINI Gateway achieves high availability by using a floating IP, which is also called a virtual IP or dynamic IP. Each server must have an IP address for communication and the IP address of a server is usually static and allocated in advance. If the server malfunctions, the IP address and the services deployed on the server are inaccessible. A floating IP address is usually a public and routable IP address that is not automatically allocated to a physical device. The project manager can temporarily allocate this dynamic IP address to one or more physical devices. The physical devices have automatically assigned static IP addresses for communicating with devices on the Intranet. This Intranet uses private addresses that are not routable. Services of physical devices on the Intranet can be identified and accessed by external networks only through the floating IP address.\nWhy Is a Floating IP Needed? #  One typical floating IP switching scenario is that, when a device bound with a floating IP address malfunctions, the floating IP address floats to another device on the network. The new device immediately replaces the faulty device to provide services externally. This creates high availability for network services. For service consumers, only the floating IP needs to be specified. Floating IPs are very useful. In certain scenarios, for example, only one service IP address is allowed for the client or SDK, which means that the IP address must be highly available. INFINI Gateway can effectively solve this problem. When two independent INFINI Gateway servers are used, you are advised to deploy them on independent physical servers. The two INFINI Gateways work in dual-node hot standby mode. If any of the gateways malfunction, front-end services can still be accessed.\nEnabling Floating IP #  To enable the floating IP feature of INFINI Gateway, modify the gateway.yml configuration file by adding the following configuration:\nfloating_ip: enabled: true INFINI Gateway can automatically detect NIC device information and bind the virtual IP address to the Intranet communication port. It is very intelligent and easy to use. By default, the IP address to be listened to is *.*.*.234 in the network segment, to which the machine belongs. Assume that the physical IP address of the machine is 192.168.3.35. The default floating IP address is 192.168.3.234. This default IP address is only used to facilitate configuration and quick startup. If you need to use a user-defined floating IP address, supplement complete parameters.\nRelated Parameter Settings #  The following is an example of configuration parameters about floating IP:\nfloating_ip: enabled: true ip: 192.168.3.234 netmask: 255.255.255.0 interface: en1 The parameters are described as follows:\n   Name Type Description     enabled bool Whether floating IP is enabled, which is set to false by default.   interface string NIC device name. If this parameter is not specified, the name of the first device that listens to the first non-local address is selected. If a server has multiple NIC cards, you are advised to manually set this parameter.   ip string Listened floating IP address, which is *.*.*.234 in the network segment, to which the current physical NIC belongs. You are advised to manually set the floating IP address. The floating IP address cannot conflict with an existing IP address.   netmask string Subnet mask of the floating IP address, which is the subnet mask of the NIC or 255.255.255.0 by default.    "});index.add({'id':5,'href':'/docs/getting-started/install/','title':"Installing the Gateway",'section':"Quick Start",'content':"Installing the Gateway #  INFINI Gateway supports mainstream operating systems and platforms. The program package is small, with no extra external dependency. So, the gateway can be installed very rapidly.\nInstallation Demo #    Downloading #  Select a package for downloading in the following URL based on your operating system and platform:\n http://download.infinilabs.com/\nContainer Deployment #  INFINI Gateway also supports Docker container deployment.\nLearn More  Verifying the Installation #  After downloading and decompressing INFINI Gateway installation package, run the following command to check whether the installation package is effective:\n✗ ./bin/gateway -v gateway 1.0.0_SNAPSHOT 2021-01-03 22:45:28 6a54bb2 If the above version information is displayed, the gateway program is in good condition.\nStarting the Gateway #  Run the gateway program as an administrator to start INFINI Gateway, as follows:\n➜ sudo ./bin/gateway ___ _ _____ __ __ __ _ / _ \\ /_\\ /__ \\/__\\/ / /\\ \\ \\/_\\ /\\_/\\ / /_\\///_\\\\ / /\\/_\\ \\ \\/ \\/ //_\\\\\\_ _/ / /_\\\\/ _ \\/ / //__ \\ /\\ / _ \\/ \\ \\____/\\_/ \\_/\\/ \\__/ \\/ \\/\\_/ \\_/\\_/ [GATEWAY] A light-weight, powerful and high-performance elasticsearch gateway. [GATEWAY] 1.0.0_SNAPSHOT, 4daf6e9, Mon Jan 11 11:40:44 2021 +0800, medcl, add response_header_filter [01-11 16:43:31] [INF] [instance.go:24] workspace: data/gateway/nodes/0 [01-11 16:43:31] [INF] [api.go:255] api server listen at: http://0.0.0.0:2900 [01-11 16:43:31] [INF] [runner.go:59] pipeline: primary started with 1 instances [01-11 16:43:31] [INF] [runner.go:59] pipeline: nodes_index started with 1 instances [01-11 16:43:31] [INF] [entry.go:262] entry [es_gateway] listen at: https://0.0.0.0:8000 [01-11 16:43:32] [INF] [floating_ip.go:170] floating_ip listen at: 192.168.3.234, echo port: 61111 [01-11 16:43:32] [INF] [app.go:254] gateway now started. If the above startup information is displayed, the gateway is running successfully and listening on the responding port.\nAccessing the Gateway #  The back-end Elasticsearch service can be accessed using a browser or other clients through the gateway that serves as a proxy:\nShutting Down the Gateway #  To shut down INFINI Gateway, hold down Ctrl+C. The following information will be displayed:\n^C [GATEWAY] got signal: interrupt, start shutting down [01-11 16:44:41] [INF] [app.go:303] gateway now terminated. [GATEWAY] 1.0.0_SNAPSHOT, uptime: 1m10.550336s Thanks for using GATEWAY, have a good day! Running in the Background #  To run INFINI Gateway as a background task, run the following command:\n➜ sudo ./bin/gateway -daemon -pidfile=/tmp/gateway.pid ___ _ _____ __ __ __ _ / _ \\ /_\\ /__ \\/__\\/ / /\\ \\ \\/_\\ /\\_/\\ / /_\\///_\\\\ / /\\/_\\ \\ \\/ \\/ //_\\\\\\_ _/ / /_\\\\/ _ \\/ / //__ \\ /\\ / _ \\/ \\ \\____/\\_/ \\_/\\/ \\__/ \\/ \\/\\_/ \\_/\\_/ [GATEWAY] A light-weight, powerful and high-performance elasticsearch gateway. [GATEWAY] 1.0.0_SNAPSHOT, aa6f614, Mon Jan 11 14:48:17 2021 +0800, medcl, add request_client_ip_filter [GATEWAY] started in background, pid: 16474 You can shut down INFINI Gateway easily through PID as follows:\nsudo kill `cat /tmp/gateway.pid` System Service #  To run the data platform of INFINI Gateway as a background task, run the following commands:\n➜ ./gateway -service install Success ➜ ./gateway -service start Success Unloading the service is simple. To unload the service, run the following commands:\n➜ ./gateway -service stop Success ➜ ./gateway -service uninstall Success INFINI Gateway has been completely installed. Next, configure the gateway.\nConfiguring INFINI Gateway  "});index.add({'id':6,'href':'/docs/tutorial/request-logging/','title':"Query Request Log Analysis",'section':"Tutorials",'content':"Query Request Log Analysis #  INFINI Gateway can track and record all requests that pass through the gateway and analyze requests sent to Elasticsearch, to figure out request performance and service running status.\nSetting a Gateway Router #  To enable the query log analysis of INFINI Gateway, configure the tracing_flow parameter on the router and set a flow to log requests.\nrouter: - name: default tracing_flow: request_logging default_flow: cache_first In the above configuration, one router named default is defined, the default request flow is cache_first, and the flow for logging is request_logging.\nDefining a Log Flow #  The log processing flow request_logging is defined as follows:\nflow: - name: request_logging filter: - request_path_filter: must_not: # any match will be filtered prefix: - /favicon.ico - request_header_filter: exclude: - app: kibana # in order to filter kibana's access log, config `elasticsearch.customHeaders: { \u0026quot;app\u0026quot;: \u0026quot;kibana\u0026quot; }` to your kibana's config `/config/kibana.yml` - logging: queue_name: request_logging The above flow uses several filters:\n The request_path_filter filters out invalid /favicon.ico requests. The request_header_filter filters out requests from Kibana. The logging filter logs requests to the local disk array request_logging so that the pipeline consumes and creates indexes.  Defining a Log Pipeline #  INFINI Gateway uses a pipeline task to asynchronously consume logs and create indexes. The configuration is as follows:\npipeline: - name: request_logging_index auto_start: true keep_running: true processor: - json_indexing: index_name: \u0026quot;gateway_requests\u0026quot; elasticsearch: \u0026quot;dev\u0026quot; input_queue: \u0026quot;request_logging\u0026quot; idle_timeout_in_seconds: 1 worker_size: 1 bulk_size_in_mb: 10 #in MB In the above configuration, one processing pipeline named request_logging_index is defined, a consumption disk queue named request_logging, an index target cluster dev, and an index named gateway_requests are set, one work thread is used, and the batch submission size is set as 10 MB.\nDefining an Index Cluster #  Configure an index cluster as follows:\nelasticsearch: - name: dev enabled: true endpoint: https://192.168.3.98:9200 # if your elasticsearch is using https, your gateway should be listen on as https as well basic_auth: #used to discovery full cluster nodes, or check elasticsearch's health and versions username: elastic password: pass discovery: # auto discovery elasticsearch cluster nodes enabled: true refresh: enabled: true In the above configuration, one Elasticsearch cluster named dev is defined and the Elastic module is enabled to process automatic configuration of the cluster.\nConfiguring an Index Template #  Configure an index template for the Elasticsearch cluster. Run the following commands on the dev cluster to create a log index template.\n Expand to View Elasticsearch Template Definition ...  PUT _template/.infini-gateway-default { \u0026quot;order\u0026quot;: 100000, \u0026quot;index_patterns\u0026quot;: [ \u0026quot;gateway_requests\u0026quot; ], \u0026quot;settings\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;max_result_window\u0026quot;: \u0026quot;10000000\u0026quot;, \u0026quot;number_of_shards\u0026quot;: \u0026quot;1\u0026quot; } }, \u0026quot;mappings\u0026quot;: { \u0026quot;dynamic_templates\u0026quot;: [ { \u0026quot;strings\u0026quot;: { \u0026quot;mapping\u0026quot;: { \u0026quot;ignore_above\u0026quot;: 256, \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;match_mapping_type\u0026quot;: \u0026quot;string\u0026quot; } } ], \u0026quot;properties\u0026quot;: { \u0026quot;request\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;body\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; } } }, \u0026quot;response\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;body\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; } } } } }, \u0026quot;aliases\u0026quot;: {} }     Configuring the Index Lifecycle #   Expand to View the Index Lifecycle Definition ...  PUT _ilm/policy/30days-retention { \u0026quot;policy\u0026quot;: { \u0026quot;phases\u0026quot;: { \u0026quot;hot\u0026quot;: { \u0026quot;min_age\u0026quot;: \u0026quot;0ms\u0026quot;, \u0026quot;actions\u0026quot;: { \u0026quot;rollover\u0026quot;: { \u0026quot;max_age\u0026quot;: \u0026quot;30d\u0026quot;, \u0026quot;max_size\u0026quot;: \u0026quot;50gb\u0026quot; }, \u0026quot;set_priority\u0026quot;: { \u0026quot;priority\u0026quot;: 100 } } }, \u0026quot;warm\u0026quot;: { \u0026quot;actions\u0026quot;: { \u0026quot;forcemerge\u0026quot;: { \u0026quot;max_num_segments\u0026quot;: 1 }, \u0026quot;set_priority\u0026quot;: { \u0026quot;priority\u0026quot;: 50 } } }, \u0026quot;cold\u0026quot;: { \u0026quot;min_age\u0026quot;: \u0026quot;3d\u0026quot;, \u0026quot;actions\u0026quot;: { \u0026quot;allocate\u0026quot;: { \u0026quot;number_of_replicas\u0026quot;: 1, \u0026quot;include\u0026quot;: {}, \u0026quot;exclude\u0026quot;: {}, \u0026quot;require\u0026quot;: { \u0026quot;box_type\u0026quot;: \u0026quot;warm\u0026quot; } }, \u0026quot;set_priority\u0026quot;: { \u0026quot;priority\u0026quot;: 0 } } }, \u0026quot;delete\u0026quot;: { \u0026quot;min_age\u0026quot;: \u0026quot;30d\u0026quot;, \u0026quot;actions\u0026quot;: { \u0026quot;delete\u0026quot;: { \u0026quot;delete_searchable_snapshot\u0026quot;: true } } } } } } PUT _template/gateway_requests-rollover { \u0026quot;order\u0026quot; : 100000, \u0026quot;index_patterns\u0026quot; : [ \u0026quot;gateway_requests-*\u0026quot; ], \u0026quot;settings\u0026quot; : { \u0026quot;index\u0026quot; : { \u0026quot;format\u0026quot; : \u0026quot;7\u0026quot;, \u0026quot;lifecycle\u0026quot; : { \u0026quot;name\u0026quot; : \u0026quot;30days-retention\u0026quot;, \u0026quot;rollover_alias\u0026quot; : \u0026quot;gateway_requests\u0026quot; }, \u0026quot;codec\u0026quot; : \u0026quot;best_compression\u0026quot;, \u0026quot;routing\u0026quot; : { \u0026quot;allocation\u0026quot; : { \u0026quot;require\u0026quot; : { \u0026quot;box_type\u0026quot; : \u0026quot;hot\u0026quot; }, \u0026quot;total_shards_per_node\u0026quot; : \u0026quot;1\u0026quot; } }, \u0026quot;number_of_shards\u0026quot; : \u0026quot;1\u0026quot; } }, \u0026quot;mappings\u0026quot; : { \u0026quot;dynamic_templates\u0026quot; : [ { \u0026quot;strings\u0026quot; : { \u0026quot;mapping\u0026quot; : { \u0026quot;ignore_above\u0026quot; : 256, \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;match_mapping_type\u0026quot; : \u0026quot;string\u0026quot; } } ] }, \u0026quot;aliases\u0026quot; : { } } DELETE gateway_requests-00001 PUT gateway_requests-00001 { \u0026quot;settings\u0026quot;: { \u0026quot;index.lifecycle.rollover_alias\u0026quot;:\u0026quot;gateway_requests\u0026quot; , \u0026quot;refresh_interval\u0026quot;: \u0026quot;5s\u0026quot; }, \u0026quot;aliases\u0026quot;:{ \u0026quot;gateway_requests\u0026quot;:{ \u0026quot;is_write_index\u0026quot;:true } } }     Importing the Dashboard #  Download the latest dashboard INFINI-Gateway-7.9.2-2021-01-15.ndjson.zip for Kibana 7.9 and import it into Kibana of the dev cluster as follows:\nStarting the Gateway #  Start the gateway.\n➜ ./bin/gateway ___ _ _____ __ __ __ _ / _ \\ /_\\ /__ \\/__\\/ / /\\ \\ \\/_\\ /\\_/\\ / /_\\///_\\\\ / /\\/_\\ \\ \\/ \\/ //_\\\\\\_ _/ / /_\\\\/ _ \\/ / //__ \\ /\\ / _ \\/ \\ \\____/\\_/ \\_/\\/ \\__/ \\/ \\/\\_/ \\_/\\_/ [GATEWAY] A light-weight, powerful and high-performance elasticsearch gateway. [GATEWAY] 1.0.0_SNAPSHOT, a17be4c, Wed Feb 3 00:12:02 2021 +0800, medcl, add extra retry for bulk_indexing [02-03 13:51:35] [INF] [instance.go:24] workspace: data/gateway/nodes/0 [02-03 13:51:35] [INF] [api.go:255] api server listen at: http://0.0.0.0:2900 [02-03 13:51:35] [INF] [runner.go:59] pipeline: request_logging_index started with 1 instances [02-03 13:51:35] [INF] [entry.go:267] entry [es_gateway] listen at: http://0.0.0.0:8000 [02-03 13:51:35] [INF] [app.go:297] gateway now started. Modifying Application Configuration #  Replace the Elasticsearch address with the gateway address for applications directed to the Elasticsearch address (such as Beats, Logstash, and Kibana). Assume that the gateway IP address is 192.168.3.98. Modify the Kibana configuration as follows:\n# The Kibana server's name. This is used for display purposes. #server.name: \u0026quot;your-hostname\u0026quot; # The URLs of the Elasticsearch instances to use for all your queries. elasticsearch.hosts: [\u0026quot;https://192.168.3.98:8000\u0026quot;] elasticsearch.customHeaders: { \u0026quot;app\u0026quot;: \u0026quot;kibana\u0026quot; } # When this setting's value is true Kibana uses the hostname specified in the server.host # setting. When the value of this setting is false, Kibana uses the hostname of the host # that connects to this Kibana instance. #elasticsearch.preserveHost: true # Kibana uses an index in Elasticsearch to store saved searches, visualizations and # dashboards. Kibana creates a new index if the index doesn't already exist. #kibana.index: \u0026quot;.kibana\u0026quot; # The default application to load. #kibana.defaultAppId: \u0026quot;home\u0026quot; Save the configuration and restart Kibana.\nChecking the Results #  All requests that access Elasticsearch through the gateway can be monitored.\n"});index.add({'id':7,'href':'/docs/references/entry/','title':"Service Entry",'section':"Reference",'content':"Service Entry #  Defining an Entry #  Each gateway must expose at least one service entrance to receive operation requests of services. In INFINI Gateway, the service entrance is called an entry, which can be defined using the following parameters:\nentry: - name: es_gateway enabled: true router: default network: binding: 0.0.0.0:8000 reuse_port: true tls: enabled: false The network.binding parameter can be used to specify the IP address and port to be bound and listened to after the service is started. INFINI Gateway supports port reuse, that is, multiple INFINI Gateways can share the same IP address and port. In this way, server resources can be fully utilized and the configuration of different gateway processes can be modified dynamically (you can start multiple processes, and then restart the processes in sequence after modifying the configuration), without interrupting normal client requests.\nFor each request sent to the entry, requested traffic is routed by router. Rules are defined for router separately so that the rules are used in different entry settings. In entry, the router parameter can be used to specify the router rules to be used and default is defined here.\nTLS Configuration #  TLS transmission encryption can be seamlessly enabled on INFINI Gateway. You can switch to HTTPS communication mode by setting tls.enabled to true. INFINI Gateway can automatically generate certification files.\nINFINI Gateway also allows you to define the path of the certification file. The configuration is as follows:\nentry: - name: es_gateway enabled: true router: default network: binding: 0.0.0.0:8000 reuse_port: true tls: enabled: true cert_file: /etc/ssl.crt key_file: /etc/ssl.key skip_insecure_verify: false Multiple Services #  INFINI Gateway can listen on multiple service entries at the same time. The listened address, protocol, and router of each service entry can be separately defined to meet different service requirements. The following shows a configuration example.\nentry: - name: es_ingest enabled: true router: ingest_router network: binding: 0.0.0.0:8000 - name: es_search enabled: true router: search_router network: binding: 0.0.0.0:9000 The above example defines a service entry named es_ingest to listen on the address 0.0.0.0:8000, and all requests are processed through ingest_router. In the example, one es_search service is also defined, the listening port is 9000, and search_router is used for request processing to implement read/write separation of services. In addition, different service entries can be defined for different back-end Elasticsearch clusters, and the gateway can forward requests as a proxy.\nParameter Description #     Name Type Description     name string Name of a service entry   enabled bool Whether the entry is enabled   max_concurrency int Maximum concurrency connection number, which is 10000 by default.   router string Router name   network object Relevant network configuration   tls object TLS secure transmission configuration   network.host string Network address listened to by the service, for example, 192.168.3.10   network.port int Port address listened to by the service, for example, 8000   network.binding string Network binding address listened to by the service, for example, 0.0.0.0:8000   network.publish string External access address listened to by the service, for example, 192.168.3.10:8000   network.reuse_port bool Whether to reuse the network port for multi-process port sharing   network.skip_occupied_port bool Whether to automatically skip occupied ports   tls.enabled bool Whether TLS secure transmission is enabled   tls.cert_file string Path to the public key of the TLS security certificate   tls.key_file string Path to the private key of the TLS security certificate   tls.skip_insecure_verify bool Whether to ignore TLS certificate verification    "});index.add({'id':8,'href':'/docs/getting-started/configuration/','title':"Configuring the Gateway",'section':"Quick Start",'content':"Configuration #  The configuration of INFINI Gateway can be modified in multiple ways.\nCLI Parameters #  INFINI Gateway provides the following CLI parameters:\n✗ ./bin/gateway --help Usage of ./bin/gateway: -config string the location of config file, default: gateway.yml (default \u0026quot;gateway.yml\u0026quot;) -cpu int the number of CPUs to use (default -1) -cpuprofile string write cpu profile to this file -daemon run in background as daemon -debug run in debug mode, gateway will quit with panic error -log string the log level,options:trace,debug,info,warn,error (default \u0026quot;info\u0026quot;) -memprofile string write memory profile to this file -pidfile string pidfile path (only for daemon mode) -pprof string enable and setup pprof/expvar service, eg: localhost:6060 , the endpoint will be: http://localhost:6060/debug/pprof/ and http://localhost:6060/debug/vars -v version The parameters are described as follows:\n config: Specifies the name of a configuration file. The default configuration file name is gateway.yml in the directory where the currently executed command is located. If your configuration file is stored elsewhere, you can specify the parameter to select it. daemon: Switches the gateway to the background. It needs to be used jointly with pidfile to save the process ID and facilitate subsequent process operations.  Configuration File #  Most of the configuration of INFINI Gateway can be completed using gateway.yml. After the configuration is modified, the gateway program needs to be restarted to make the configuration take effect.\nDefining an Entry #  Each gateway must expose at least one service entrance to receive operation requests of services. In INFINI Gateway, the service entrance is called an entry, which can be defined using the following parameters:\nentry: - name: es_gateway enabled: true router: default network: binding: 0.0.0.0:8000 The above configuration defines one service entry named es_gateway, the address listened to is 0.0.0.0:8000, and one router named default is used to process requests.\nDefining a Router #  INFINI Gateway judges the flow direction based on routers. A typical example of router configuration is as follows:\nrouter: - name: default default_flow: cache_first This example defines one router named default, which is also the main flow for service handling. Request forwarding, filtering, caching, and other operations are performed in this flow.\nDefining a Flow #  One request flow defines a series of work units for request handling. It adopts a typical pipeline work mode. One typical configuration example is as follows:\nflow: - name: cache_first filter: - get_cache: - elasticsearch: elasticsearch: prod - set_cache: The configuration example defines a flow named cache_first, which uses three different filters: get_cache, elasticsearch, and set_cache. These filters are executed in their configuration sequence. Note that each filter name must be appended with one colon (:). The processing results of the filters are as follows:\n get_cache: This filter is mainly used to get data from the cache. If the same request has been received before and data is cached in the cache, which is within the validity period, this filter can directly take and return the cached data immediately, without further processing. elasticsearch: This filter is used to forward requests to back-end Elasticsearch clusters and further transfer responses returned by Elasticsearch. set_cache: This filter caches execution results to the local memory. It has some parameter restrictions such as the status code and request size, and expiration time is set for the filter so that results in the cache can be used directly when the same request is received next time. It is generally used together with get_cache.  Defining a Resource #  Resources here refer to Elasticsearch back-end server resources. INFINI Gateway supports multiple Elasticsearch clusters. It can forward requests to different clusters and supports blue/green deployment and smooth evolution under canary deployment of requests. The following example shows how to define an Elasticsearch back-end resource.\nelasticsearch: - name: prod enabled: true endpoint: http://192.168.3.201:9200 discovery: enabled: true refresh: enabled: true basic_auth: username: elastic password: pass The endpoint parameter is used to set the access address for Elasticsearch. If identity authentication is enabled for Elasticsearch, you can use basic_auth to specify the username and password and the user must have the permission to obtain cluster status information. The discover parameter is used to enable automatic discovery to automatically detect the status of back-end nodes and identify new and offline nodes.\nAfter these basic configurations have been completed, INFINI Gateway can normally handle Elasticsearch requests as a proxy. For details about parameters of each component, see the Reference.\n"});index.add({'id':9,'href':'/docs/user-cases/stories/indexing_speedup_for_big_index_rebuild/','title':"How an Insurance Group Improved the Indexing Speed by 200x Times",'section':"User Cases",'content':"How an Insurance Group Improved the Indexing Speed by 200x Times #  Challenges #  A large insurance group places common database fields in Elasticsearch to improve the query performance for its policy query service. The cluster is deployed on 14 physical machines, with 4 Elasticsearch instances deployed on each physical machine. The whole cluster has more than 9 billion pieces of data. The storage size of index primary shards is close to 5 TB, and about 600 million pieces of incremental data are updated every day. Due to the service particularity, all the service data across the country is stored in one index, resulting in up to 210 shards in the single index. The bulk rebuilding task is executed in parallel by Spark. The average write speed is about 2,000–3,000 pieces per second. One incremental rebuilding operation may take 2–3 days. Service data updating causes a large delay and the lengthy rebuilding also affects service access during normal time periods. The technical team had tried hard to optimize Elasticsearch and also the Spark write end for several rounds, but did not get any progress in the indexing speed improvement.\nScenario #  The analysis shows that the cluster performance is good. However, after write requests in a single batch are received by Elasticsearch, they need to be encapsulated and forwarded according to the node where the primary shard is located. There are too many service index shards and each data node eventually gets a very small number of request documents. One bulk write request of the client is divided into hundreds of small bulk requests. According to the short board theory of the barrel, the processing speed of the slowest node slows down the whole bulk write operation. INFINI Gateway knows where a document should go to.\nINFINI Gateway is capable of splitting and merging requests in advance. It splits and merges requests in advance, sends the requests to local queues based on the target node, and then writes the requests to the target Elasticsearch cluster through the queue consumption program to convert random bulk requests into sequential requests that are precisely delivered. See the figure below.\nAfter receiving a request from Spark, INFINI Gateway first stores the request on the local disk to prevent data loss. Meanwhile, INFINI Gateway can locally calculate the routing information of each document and the target data nodes. The new data writing architecture is shown in the figure below.\nAfter INFINI Gateway is used to receive write requests from Spark, the write throughput of the entire cluster is significantly improved. Spark accomplishes the data writing task in less than 15 minutes, and it takes only 20 minutes for the gateway to receive requests and write them into Elasticsearch. The CPU resources of the server are fully utilized and all the CPU resources of each node are used.\nUser Benefits #   The Indexing Speed Is Improved by 20,000%\n After INFINI Gateway is used as the intermediate acceleration layer, the index rebuilding cycle of the group\u0026rsquo;s policy service is reduced from 2–3 days to about 20 minutes, the 600 million pieces of daily incremental data can also be rebuilt very quickly, and the peak index write QPS can exceed 300,000. In a word, INFINI Gateway greatly shortens the index rebuilding cycle, reduces data latency, enhances the consistency of online data, and ensures the normal use of the query service.\n"});index.add({'id':10,'href':'/docs/tutorial/index_diff/','title':"Index Document-Level Difference Contrast",'section':"Tutorials",'content':"Index Difference Contrast #  INFINI Gateway is able to compare differences between two different indexes in the same or different clusters. In scenarios in which application dual writes, CCR, or other data replication solutions are used, differences can be periodically compared to ensure data consistency.\nFunction Demonstration #    How Is This Feature Configured? #  Setting a Target Cluster #  Modify the gateway.yml configuration file by setting two cluster resources source and target and adding the following configuration:\nelasticsearch: - name: source enabled: true endpoint: http://localhost:9200 basic_auth: username: test password: testtest - name: target enabled: true endpoint: http://localhost:9201 basic_auth: #used to discovery full cluster nodes, or check elasticsearch's health and versions username: test password: testtest Configuring a Contrast Task #  Add a service pipeline to handle the index document pulling and contrast of two clusters as follows:\npipeline: - name: index_diff_service auto_start: true keep_running: true processor: - dag: parallel: - dump_hash: #dump es1's doc indices: \u0026quot;medcl-test\u0026quot; scroll_time: \u0026quot;10m\u0026quot; elasticsearch: \u0026quot;source\u0026quot; output_queue: \u0026quot;source_docs\u0026quot; batch_size: 10000 slice_size: 5 - dump_hash: #dump es2's doc indices: \u0026quot;medcl-test\u0026quot; scroll_time: \u0026quot;10m\u0026quot; batch_size: 10000 slice_size: 5 elasticsearch: \u0026quot;target\u0026quot; output_queue: \u0026quot;target_docs\u0026quot; end: - index_diff: diff_queue: \u0026quot;diff_result\u0026quot; buffer_size: 1 text_report: true #If data needs to be saved to Elasticsearch, disable the function and start the diff_result_ingest task of the pipeline. source_queue: 'source_docs' target_queue: 'target_docs' In the above configuration, dump_hash is concurrently used to pull the medcl-a index of the source cluster and fetch the medcl-b index of the target cluster, and output results to terminals in text form.\nOutputting Results to Elasticsearch #  If there are many difference results, you can save them to the Elasticsearch cluster, set the text_report parameter of the above index_diff processing unit to false, and add the following configuration:\npipeline: - name: diff_result_ingest auto_start: true keep_running: true processor: - json_indexing: index_name: \u0026quot;diff_result\u0026quot; elasticsearch: \u0026quot;source\u0026quot; input_queue: \u0026quot;diff_result\u0026quot; idle_timeout_in_seconds: 1 worker_size: 1 bulk_size_in_mb: 10 #in MB Finally, import the dashboard to Kibana to achieve the following effect:\n"});index.add({'id':11,'href':'/docs/references/modules/force_merge/','title':"Index Segment Merging",'section':"Functional Component",'content':"Active Merging of Index Segments #  INFINI Gateway has an index segment merging service, which can actively merge index segment files to improve query speed. The index segment merging service supports sequential processing of multiple indexes and tracks the status of the merging task, thereby preventing cluster slowdown caused by concurrent operations of massive index segment merging tasks.\nEnabling the Service #  Modify the gateway.yml configuration file by adding the following configuration:\nforce_merge: enabled: false elasticsearch: dev min_num_segments: 20 max_num_segments: 1 indices: - index_name The parameters are described as follows:\n   Name Type Description     enabled bool Whether the module is enabled, which is set to false by default.   elasticsearch string ID of an Elasticsearch cluster, on which index segment merging is performed   min_num_segments int Minimum number of shards in an index for active shard merging. The value is based on indexes.   max_num_segments int The maximum number of segment files that can be generated after segment files in a shard are merged   indices array List of indexes that need shard merging   discovery object Auto-discovery of index-related settings   discovery.min_idle_time string Minimum time span for judging whether segment merging conditions are met. The default value is 1d.   discovery.interval string Interval for detecting whether segment merging is required   discovery.rules array Index matching rules used in automatic index detection   discovery.rules.index_pattern string Pattern of indexes that need index segment file merging   discovery.rules.timestamp_fields array List of fields representing the index timestamp    "});index.add({'id':12,'href':'/docs/user-cases/stories/a_cross_region_cluster_access_locality/','title':"Nearest Cluster Access Across Two Cloud Providers",'section':"User Cases",'content':"Nearest Cluster Access Across Two Cloud Providers #  Service Requirements #  To ensure the high availability of the Elasticsearch service, Zuoyebang deploys a single Elasticsearch cluster on both Baidu Cloud and Huawei Cloud and requires that service requests be sent to the nearest cloud first.\nDeployment of a Single Elasticsearch Cluster on Dual Clouds #  The Elasticsearch cluster uses an architecture with master nodes separated from data nodes. Currently, the main cloud is used to accommodate two master nodes and the other cloud is used to accommodate another master node. The main consideration is that infrastructure failures are mostly dedicated line failures, and the overall breakdown of a provider\u0026rsquo;s cloud rarely occurs. Therefore, the main cloud is configured. When a dedicated line failure occurs, the Elasticsearch cluster on the main cloud is read/write and service traffic can be switched to the main cloud.\nThe configuration is as follows:\nFirst, complete the following settings on the master nodes:\ncluster.routing.allocation.awareness.attributes: zone_id cluster.routing.allocation.awareness.force.zone_id.values: zone_baidu,zone_huawei Then, perform the following settings on data nodes on Baidu Cloud:\nnode.attr.zone_id: zone_baidu Perform the following settings on data nodes on Huawei Cloud:\nnode.attr.zone_id: zone_huawei Indexes are created using one copy, which can ensure that the same copy of data exists on Baidu Cloud and Huawei Cloud.\nThe service access mode is shown as follows:\n Baidu Cloud service -\u0026gt; Baidu lb -\u0026gt; INFINI Gateway (Baidu Cloud) -\u0026gt; Elasticsearch (data nodes on Baidu Cloud) Huawei Cloud service -\u0026gt; Huawei lb -\u0026gt; INFINI Gateway (Huawei Cloud) -\u0026gt; Elasticsearch (data nodes on Huawei Cloud)  Configuring INFINI Gateway #  Elasticsearch uses the Preference parameter to set the request access priority. Set the default Preference parameter for requests on INFINI Gateway inside the two clouds so that requests inside each cloud are sent to data nodes in the local cloud first, thereby implementing nearest access of requests.\nThe specific configuration on INFINI Gateway inside Baidu Cloud is as follows (the configuration on INFINI Gateway inside Huawei Cloud is basically the same and is not provided here):\npath.data: data path.logs: log entry: - name: es-test enabled: true router: default network: binding: 0.0.0.0:9200 reuse_port: true router: - name: default default_flow: es-test flow: - name: es-test filter: - set_request_query_args: args: - preference -\u0026gt; _prefer_nodes:data-baidu01,data-baidu02 #Set _prefer_nodes of Preference to all Baidu data nodes so that Baidu Cloud service accesses the nodes of Baidu Cloud first, thereby avoiding cross-cloud access to the maximum extent and enabling the service to run more smoothly. when: contains: _ctx.request.path: /_search - elasticsearch: elasticsearch: default refresh: enabled: true interval: 10s roles: include: - data #Set it to data so that requests are sent only to the data node. tags: include: - zone_id: zone_baidu #Requests are forwarded only to nodes in Baidu Cloud. elasticsearch: - name: default enabled: true endpoint: http://10.10.10.10:9200 discovery: enabled: true refresh: enabled: true interval: 10s basic_auth: username: elastic password: elastic Summary and Benefits #  Retrospect of Failures Before INFINI Gateway Is Introduced #  When Baidu Cloud service accesses the Elasticsearch cluster, it pulls daily incremental data from the Hive cluster and synchronizes it to Elasticsearch. Some tasks may fail and data is synchronized again. As a result, some data is pulled from the Elasticsearch node inside Huawei Cloud to the Hive cluster of Baidu Cloud. The huge amount of data triggers an alarm about cross-cloud dedicated line traffic monitoring. Online services, MySQL, Redis, and Elasticsearch use the same dedicated line. The impact of the failures is huge. The temporary solution is to add the Preference parameter to the service modification statement so that the services only pull local cloud data, reducing the occupancy of the dedicated line. The service transformation and maintenance costs are high. In addition, DBA has worries that there are omissions in service transformation, the Preference parameter is ignored for new services, and later adjustment costs are high. These are always risk points.\nBenefits of INFINI Gateway #  After INFINI Gateway is added to the original architecture, services can preferentially access the local cloud with the service code not modified. In this way, CPU resources of the server are fully utilized and all CPU resources of each node are used.\n Author: Zhao Qing, former DBA of NetEase, mainly involved in the O\u0026amp;M of Oracle, MySQL, Redis, Elasticsearch, Tidb, OB, and other components, as well as O\u0026amp;M automation, platform-based application, and intelligence. Now he is working in Zuoyebang.\n "});index.add({'id':13,'href':'/docs/references/router/','title':"Service Router",'section':"Reference",'content':"Service Router #  INFINI Gateway judges the flow direction based on routers. A typical example of router configuration is as follows:\nrouter: - name: my_router default_flow: default_flow tracing_flow: request_logging rules: - method: - PUT - POST pattern: - \u0026quot;/_bulk\u0026quot; - \u0026quot;/{index_name}/_bulk\u0026quot; flow: - bulk_process_flow Router involves several important terms:\n Flow: Handling flow of a request. Flows can be defined in three places in a router. default_flow: Default handling flow, which is the main flow of service handling. Request forwarding, filtering, and caching are performed in this flow. tracing_flow: Flow used to track the request status. It is independent of the default_flow. This flow is used to log requests and collect statistics. rules: Requests are distributed to specific handling flows according to matching rules. Regular expressions can be used to match the methods and paths of requests.  Parameter Description #     Name Type Description     name string Route name   default_flow string Name of the default request handling flow   tracing_flow string Name of the flow used to trace a request   rules array List of routing rules to be applied in the array sequence   rules.method string Method type of a request. The GET, HEAD, POST, PUT, PATCH, DELETE, CONNECT, OPTIONS, and TRACE types are supported and * indicates any type.   rules.pattern string URL path matching rule of a request. Patterns are supported and overlapping matches are not allowed.   rules.flow string Flow to be executed after rule matching. Multiple flows can be combined and they are executed sequentially.    Pattern Syntax #     Syntax Description Example     {Variable name} Variable with a name /{name}   {Variable name:regexp} Restricts the matching rule of the variable by using a regular expression. /{name:[a-zA-Z]}   {Variable name:*} Any path after matching. It can be applied only to the end of a pattern. /{any:*}    Examples:\nPattern: /user/{user} /user/gordon match /user/you match /user/gordon/profile no match /user/ no match Pattern with suffix: /user/{user}_admin /user/gordon_admin match /user/you_admin match /user/you no match /user/gordon/profile no match /user/gordon_admin/profile no match /user/ no match Pattern: /src/{filepath:*} /src/ match /src/somefile.go match /src/subdir/somefile.go match Notes:\n A pattern must begin with /. Any match is only used as the last rule.  "});index.add({'id':14,'href':'/docs/getting-started/docker/','title':"Container Deployment",'section':"Quick Start",'content':"Container Deployment #  INFINI Gateway supports container deployment.\nInstallation Demo #    Downloading an Image #  The images of INFINI Gateway are published at the official repository of Docker. The URL is as follows:\n https://hub.docker.com/r/infinilabs/gateway\nUse the following command to obtain the latest container image:\ndocker pull infinilabs/gateway:latest Verifying the Image #  After downloading the image locally, you will notice that the container image of INFINI Gateway is very small, with a size less than 25 MB. So, the downloading is very fast.\n✗ docker images REPOSITORY TAG IMAGE ID CREATED SIZE infinilabs/gateway latest fdae74b64e1a 47 minutes ago 23.5MB Creating Configuration #  Create a configuration file gateway.yml to perform basic configuration as follows:\npath.data: data path.logs: log entry: - name: my_es_entry enabled: true router: my_router max_concurrency: 200000 network: binding: 0.0.0.0:8000 flow: - name: simple_flow filter: - elasticsearch: elasticsearch: dev router: - name: my_router default_flow: simple_flow elasticsearch: - name: dev enabled: true endpoint: http://localhost:9200 basic_auth: username: test password: testtest Note: In the above configuration, replace the Elasticsearch configuration with the actual server connection address and authentication information.\nStarting the Gateway #  Use the following command to start the INFINI Gateway container:\ndocker run -p 2900:2900 -p 8000:8000 -v=`pwd`/gateway.yml:/gateway.yml infinilabs/gateway:latest Verifying the Gateway #  If the gateway runs properly, the following information is displayed:\n➜ /tmp docker run -p 2900:2900 -p 8000:8000 -v=`pwd`/gateway.yml:/gateway.yml infinilabs/gateway:latest ___ _ _____ __ __ __ _ / _ \\ /_\\ /__ \\/__\\/ / /\\ \\ \\/_\\ /\\_/\\ / /_\\///_\\\\ / /\\/_\\ \\ \\/ \\/ //_\\\\\\_ _/ / /_\\\\/ _ \\/ / //__ \\ /\\ / _ \\/ \\ \\____/\\_/ \\_/\\/ \\__/ \\/ \\/\\_/ \\_/\\_/ [GATEWAY] A light-weight, powerful and high-performance elasticsearch gateway. [GATEWAY] 1.0.0_SNAPSHOT, b61758c, Mon Dec 28 14:32:02 2020 +0800, medcl, no panic by default [12-30 05:26:41] [INF] [instance.go:24] workspace: data/gateway/nodes/0 [12-30 05:26:41] [INF] [runner.go:59] pipeline: primary started with 1 instances [12-30 05:26:41] [INF] [entry.go:257] entry [es_gateway] listen at: http://0.0.0.0:8000 [12-30 05:26:41] [INF] [app.go:247] gateway now started. [12-30 05:26:45] [INF] [reverseproxy.go:196] elasticsearch [prod] endpoints: [] =\u0026gt; [192.168.3.201:9200] If you want the container to run in the background, append the parameter -d as follows:\ndocker run -d -p 2900:2900 -p 8000:8000 -v=`pwd`/gateway.yml:/gateway.yml infinilabs/gateway:latest Access the URL http://localhost:8000/ from the CLI or browser. The Elasticsearch can be accessed normally. See the following information.\n➜ /tmp curl -v http://localhost:8000/ * Trying ::1... * TCP_NODELAY set * Connected to localhost (::1) port 8000 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; Host: localhost:8000 \u0026gt; User-Agent: curl/7.64.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: INFINI \u0026lt; Date: Wed, 30 Dec 2020 05:12:39 GMT \u0026lt; Content-Type: application/json; charset=UTF-8 \u0026lt; Content-Length: 480 \u0026lt; UPSTREAM: 192.168.3.201:9200 \u0026lt; { \u0026quot;name\u0026quot; : \u0026quot;node1\u0026quot;, \u0026quot;cluster_name\u0026quot; : \u0026quot;pi\u0026quot;, \u0026quot;cluster_uuid\u0026quot; : \u0026quot;Z_HcN_6ESKWicV-eLsyU4g\u0026quot;, \u0026quot;version\u0026quot; : { \u0026quot;number\u0026quot; : \u0026quot;6.4.2\u0026quot;, \u0026quot;build_flavor\u0026quot; : \u0026quot;default\u0026quot;, \u0026quot;build_type\u0026quot; : \u0026quot;tar\u0026quot;, \u0026quot;build_hash\u0026quot; : \u0026quot;04711c2\u0026quot;, \u0026quot;build_date\u0026quot; : \u0026quot;2018-09-26T13:34:09.098244Z\u0026quot;, \u0026quot;build_snapshot\u0026quot; : false, \u0026quot;lucene_version\u0026quot; : \u0026quot;7.4.0\u0026quot;, \u0026quot;minimum_wire_compatibility_version\u0026quot; : \u0026quot;5.6.0\u0026quot;, \u0026quot;minimum_index_compatibility_version\u0026quot; : \u0026quot;5.0.0\u0026quot; }, \u0026quot;tagline\u0026quot; : \u0026quot;You Know, for Search\u0026quot; } * Connection #0 to host localhost left intact * Closing connection 0 Docker Compose #  You can also use docker compose to manage container instances. Create one docker-compose.yml file as follows:\nversion: \u0026quot;3.5\u0026quot; services: infini-gateway: image: infinilabs/gateway:latest ports: - 2900:2900 - 8000:8000 container_name: \u0026quot;infini-gateway\u0026quot; volumes: - ../gateway.yml:/gateway.yml volumes: dist: In the directory where the configuration file resides, run the following command to start INFINI Gateway.\n➜ docker-compose up Starting infini-gateway ... done Attaching to infini-gateway infini-gateway | ___ _ _____ __ __ __ _ infini-gateway | / _ \\ /_\\ /__ \\/__\\/ / /\\ \\ \\/_\\ /\\_/\\ infini-gateway | / /_\\///_\\\\ / /\\/_\\ \\ \\/ \\/ //_\\\\\\_ _/ infini-gateway | / /_\\\\/ _ \\/ / //__ \\ /\\ / _ \\/ \\ infini-gateway | \\____/\\_/ \\_/\\/ \\__/ \\/ \\/\\_/ \\_/\\_/ infini-gateway | infini-gateway | [GATEWAY] A light-weight, powerful and high-performance elasticsearch gateway. infini-gateway | [GATEWAY] 1.0.0_SNAPSHOT, b61758c, Mon Dec 28 14:32:02 2020 +0800, medcl, no panic by default infini-gateway | [12-30 13:24:16] [INF] [instance.go:24] workspace: data/gateway/nodes/0 infini-gateway | [12-30 13:24:16] [INF] [api.go:244] api server listen at: http://0.0.0.0:2900 infini-gateway | [12-30 13:24:16] [INF] [runner.go:59] pipeline: primary started with 1 instances infini-gateway | [12-30 13:24:16] [INF] [entry.go:257] entry [es_gateway] listen at: http://0.0.0.0:8000 infini-gateway | [12-30 13:24:16] [INF] [app.go:247] gateway now started. "});index.add({'id':15,'href':'/docs/getting-started/optimization/','title':"System Optimization",'section':"Quick Start",'content':"System Optimization #  The operating system of the server where INFINI Gateway is installed needs to be optimized to ensure that INFINI Gateway runs in the best possible state. The following uses Linux as an example.\nSystem Parameters #  vi /etc/security/limits.conf\n* soft nofile 1024000 * hard nofile 1024000 * soft memlock unlimited * hard memlock unlimited root soft nofile 1024000 root hard nofile 1024000 root soft memlock unlimited Kernel Optimization #  vi /etc/sysctl.conf\nnet.ipv4.ip_forward = 1 net.ipv4.conf.default.accept_redirects = 0 net.ipv4.conf.default.rp_filter = 1 net.ipv4.conf.all.accept_redirects = 0 net.ipv4.conf.all.send_redirects = 0 net.ipv4.ip_nonlocal_bind=1 net.ipv4.tcp_tw_reuse=1 net.ipv4.tcp_timestamps=1 net.ipv4.tcp_syncookies=1 net.ipv4.tcp_max_syn_backlog=65535 net.ipv4.tcp_synack_retries=0 net.core.somaxconn=32768 net.core.netdev_max_backlog=65535 net.core.rmem_max=4194304 net.core.wmem_max=4194304 fs.file-max=10485760 vm.max_map_count=262144 Run the following command to check whether configuration parameters are valid.\nsysctl -p Restart the operating system to make the configuration take effect.\n"});index.add({'id':16,'href':'/docs/user-cases/','title':"User Cases",'section':"Docs",'content':"User Cases #  Who Is Using INFINI Gateway? #  If you are using INFINI Gateway and feel it good enough to tell others you are using it, please leave a message in Github Discussion to let us know. Thank you for your support and encouragement.\nCustomers in China #                             "});index.add({'id':17,'href':'/docs/references/flow/','title':"Handling Flow",'section':"Reference",'content':"Handling Flow #  Flow Definition #  Requests received by each gateway are handled through a series of processes and then results are returned to the client. A process is called a flow in INFINI Gateway. See the following example.\nflow: - name: hello_world filter: - echo: str: \u0026quot;hello gateway\\n\u0026quot; repeat: 1 - name: not_found filter: - echo: str: '404 not found\\n' repeat: 1 The above example defines two flows: hello_world and not_found. Each flow uses a filter named echo to output a string. A series of filters can be defined in each flow and they are executed in the defined sequence.\nSyntax Description #  INFINI Gateway defines a flow in the stipulated format and supports flexible conditional parameters for logical judgment. The specific format is defined as follows:\nflow: - name: \u0026lt;flow_name\u0026gt; filter: - \u0026lt;filter_name\u0026gt;: when: \u0026lt;condition\u0026gt; \u0026lt;parameters\u0026gt; - \u0026lt;filter_name\u0026gt;: when: \u0026lt;condition\u0026gt; \u0026lt;parameters\u0026gt; ... In the format defined above, filter_name indicates the name of a filter, which is used to execute a specific task. condition below when is used to define specific conditional parameters for executing the task, and the filter task is skipped when the conditions are not met. In parameters, parameters related to the filter are set, and the parameters are separated by the line feed character.\nConditional Judgment #  Complex logical judgments can be defined in a flow of INFINI Gateway so that a filter can be executed only when certain conditions are met. See the following example.\nfilter: - if: \u0026lt;condition\u0026gt; then: - \u0026lt;filter_name\u0026gt;: \u0026lt;parameters\u0026gt; - \u0026lt;filter_name\u0026gt;: \u0026lt;parameters\u0026gt; ... else: - \u0026lt;filter_name\u0026gt;: \u0026lt;parameters\u0026gt; - \u0026lt;filter_name\u0026gt;: \u0026lt;parameters\u0026gt; ... Parameter Description #     Name Type Description     then array A series of filters to be executed only when conditions defined in condition are met.   else array A set of filters to be executed only when the conditions are not met. You do not have to set it.    You can use if to make conditional judgment and logical selection in the case of multiple filters and use when to determine whether to execute a single filter.\nCondition Type #  For various condition defined in a flow, you can use the current request context to judge whether a specific condition is met so as to achieve logical processing. The conditions support the combination of Boolean expressions (AND, NOT, and OR). The complete list of condition types is as follows:\n equals contains regexp range network has_fields in queue_has_lag consumer_has_lag cluster_available or and not  equals #  The equals condition is used to judge whether the content of a field is the specified value. It is used for the exact match of characters and digits.\nThe following example determines whether the request method is of the GET type and _ctx is a specific keyword for accessing the request context:\nequals: _ctx.request.method: GET contains #  The contains condition is used to judge whether the content of a field contains a specific character value. It supports only character fields.\nThe following example judges whether the returned response body contains an error keyword:\ncontains: _ctx.response.body: \u0026quot;error\u0026quot; regexp #  The regexp condition is used to judge whether the content of a field meets the matching rules of a regular expression. It supports only character fields.\nThe following example judges whether the request URI is a query request:\nregexp: _ctx.request.uri: \u0026quot;.*/_search\u0026quot; range #  The range condition is used to judge whether the value of a field meets a specific range. It supports the lt, lte, gt, and gte types and only numeric fields are supported.\nThe following example judges the range of the status code:\nrange: _ctx.response.code: gte: 400 The following combination example judges the range of the response byte size:\nrange: _ctx.request.body_length.gte: 100 _ctx.request.body_length.lt: 5000 network #  If the value of a field is an IP address, you can use the network condition to judge whether the field meets a specific network range, whether it supports standard IPv4 or IPv6, whether it supports the classless inter-domain routing (CIDR) expression, or whether it uses an alias in the following range:\n   Name Description     loopback Matches the local loopback network address. Range: 127.0.0.0/8 or ::1/128.   unicast Matches global unicast addresses defined in RFC 1122, RFC 4632, and RFC 4291, except the IPv4 broadcast address (255.255.255.255) but including private address ranges.   multicast Matches the broadcast address.   interface_local_multicast Matches the local multicast address of an IPv6 interface.   link_local_unicast Matches the link-local unicast address.   link_local_multicast Matches the link-local broadcast address.   private Matches the private address range defined in RFC 1918 (IPv4) and RFC 4193 (IPv6).   public Matches public addresses other than the local address, unspecified address, IPv4 broadcast address, link-local unicast address, link-local multicast address, interface local multicast address, or private address.   unspecified Matches an unspecified address (IPv4 address 0.0.0.0 or IPv6 address ::).    The following example matches the local network address:\nnetwork: _ctx.request.client_ip: private The following example specifies a subnet:\nnetwork: _ctx.request.client_ip: '192.168.3.0/24' An array is supported and it is judged that the condition is met when any value in the array is met.\nnetwork: _ctx.request.client_ip: ['192.168.3.0/24', '10.1.0.0/8', loopback] has_fields #  You can use the has_fields condition to judge whether a field exists. It supports the use of one or more character fields. See the following example:\nhas_fields: ['_ctx.request.user'] in #  You can use the in condition to judge whether a field has any value in a specified array. It supports a single field and the character and numeric types.\nThe following example judges the returned status code.\nin: _ctx.response.status: [ 403,404,200,201 ] queue_has_lag #  The queue_has_lag condition is used to judge whether one or more local disk queues are stacked with messages.\nqueue_has_lag: [ \u0026quot;prod\u0026quot;, \u0026quot;prod-500\u0026quot; ] If you want to set the depth of a queue to be greater than a specified depth, add \u0026gt;queue depth to the end of the queue name. See the following example:\nqueue_has_lag: [ \u0026quot;prod\u0026gt;10\u0026quot;, \u0026quot;prod-500\u0026gt;10\u0026quot; ] The above example shows that the condition is met only when the queue depth exceeds 10.\nconsumer_has_lag #  The consumer_has_lag condition is used to judge whether delay and message stacking occur in the consumer of a queue.\nconsumer_has_lag: queue: \u0026quot;primary-partial-success_bulk_requests\u0026quot; group: \u0026quot;my-group\u0026quot; consumer: \u0026quot;my-consumer-1\u0026quot; cluster_available #  The cluster_available condition is used to judge the service availability of one or more Elasticsearch clusters. See the following example:\ncluster_available: [\u0026quot;prod\u0026quot;] or #  The or condition is used to combine multiple optional conditions in the following format:\nor: - \u0026lt;condition1\u0026gt; - \u0026lt;condition2\u0026gt; - \u0026lt;condition3\u0026gt; ... See the following example:\nor: - equals: _ctx.response.code: 304 - equals: _ctx.response.code: 404 and #  The and condition is used to combine multiple necessary conditions in the following format:\nand: - \u0026lt;condition1\u0026gt; - \u0026lt;condition2\u0026gt; - \u0026lt;condition3\u0026gt; ... See the following example:\nand: - equals: _ctx.response.code: 200 - equals: _ctx.status: OK You can combine the and and or conditions flexibly. See the following example:\nor: - \u0026lt;condition1\u0026gt; - and: - \u0026lt;condition2\u0026gt; - \u0026lt;condition3\u0026gt; not #  If you want to negate a condition, use the not condition in the following format:\nnot: \u0026lt;condition\u0026gt; See the following example:\nnot: equals: _ctx.status: OK "});index.add({'id':18,'href':'/docs/references/elasticsearch/','title':"Elasticsearch",'section':"Reference",'content':"Elasticsearch #  Defining a Resource #  INFINI Gateway supports multi-cluster access and different versions. Each cluster serves as one Elasticsearch back-end resource and can be subsequently used by INFINI Gateway in multiple locations. See the following example.\nelasticsearch: - name: local enabled: true endpoint: https://127.0.0.1:9200 - name: dev enabled: true endpoint: https://192.168.3.98:9200 basic_auth: username: elastic password: pass - name: prod enabled: true endpoint: http://192.168.3.201:9200 discovery: enabled: true refresh: enabled: true interval: 10s basic_auth: username: elastic password: pass The above example defines a local development test cluster named local and a development cluster named dev. Authentication is enabled in the development cluster, in which corresponding usernames and passwords are also defined. In addition, one production cluster named prod is defined, and the auto node topology discovery and update of the cluster are enabled through the discovery parameter.\nParameter Description #     Name Type Description     name string Name of an Elasticsearch cluster   enabled bool Whether the cluster is enabled   endpoint string Elasticsearch access address, for example, http://localhost:9200   endpoints array List of Elasticsearch access addresses. Multiple entry addresses are supported for redundancy.   schema string Protocol type: http or https   host string Elasticsearch host, in the format of localhost:9200. Either the host or endpoint configuration mode can be used.   hosts array Elasticsearch host list. Multiple entry addresses are supported for redundancy.   request_timeout int Request timeout duration, in seconds   request_compress bool Whether to enable Gzip compression   basic_auth object Authentication information   basic_auth.username string Username   basic_auth.password string Password   discovery object Cluster discovery settings   discovery.enabled bool Whether to enable cluster topology discovery   discovery.refresh object Cluster topology update settings   discovery.refresh.enabled bool Whether to enable auto cluster topology update   discovery.refresh.interval string Interval of auto cluster topology update   traffic_control object Node-level overall traffic control of the cluster   traffic_control.max_bytes_per_node int Maximum allowable number of request bytes per second   traffic_control.max_qps_per_node int Maximum allowable number of requests per second, regardless of read or write requests    "});index.add({'id':19,'href':'/docs/references/context/','title':"Request Context",'section':"Reference",'content':"Request Context #  What Is Context #  Context is the entry for INFINI Gateway to access relevant information in the current running environment, such as the request source and configuration. You can use the _ctx keyword to access relevant fields, for example, _ctx.request.uri, which indicates the requested URL.\nEmbedded Request Context #  The embedded _ctx context objects of an HTTP request mainly include the following:\n   Name Type Description     id uint64 Unique ID of the request   tls bool Whether the request is a TLS request   remote_addr string Source IP address of the client   local_addr string Gateway local IP address   elapsed int64 Time that the request has been executed (ms)   request.* object Request description   response.* object Response description    request #  The request object has the following attributes:\n   Name Type Description     to_string string Complete HTTP request in text form   host string Accessed destination host name/domain name   method string Request type   uri string Complete URL of request   path string Request path   query_args map URL request parameter   user string Name of the user who initiates the request   header map Header parameter   body string Request body   body_json object JSON request body object   body_length int Request body length    If the request body data submitted by the client is in JSON format, you can use body_json to access the data. See the following example.\ncurl -u tesla:password -XGET \u0026quot;http://localhost:8000/medcl/_search?pretty\u0026quot; -H 'Content-Type: application/json' -d' { \u0026quot;query\u0026quot;:{ \u0026quot;bool\u0026quot;:{ \u0026quot;must\u0026quot;:[{\u0026quot;match\u0026quot;:{\u0026quot;name\u0026quot;:\u0026quot;A\u0026quot;}},{\u0026quot;match\u0026quot;:{\u0026quot;age\u0026quot;:18}}] }\t}, \u0026quot;size\u0026quot;:900, \u0026quot;aggs\u0026quot;: { \u0026quot;total_num\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;name1\u0026quot;, \u0026quot;size\u0026quot;: 1000000 } } } }' In JSON data, . is used to identify the path. If the data is an array, you can use [Subscript] to access a specified element, for example, you can use a dump filter for debugging as follows:\n - name: cache_first filter: - dump: context: - _ctx.request.body_json.size - _ctx.request.body_json.aggs.total_num.terms.field - _ctx.request.body_json.query.bool.must.[1].match.age The output is as follows:\n_ctx.request.body_json.size : 900 _ctx.request.body_json.aggs.total_num.terms.field : name1 _ctx.request.body_json.query.bool.must.[1].match.age : 18 response #  The response object has the following attributes:\n   Name Type Description     to_string string Complete HTTP response in text form   status int Request status code   header map Header parameter   content_type string Response body type   body string Response body   body_length int Response body length    "});index.add({'id':20,'href':'/docs/troubleshooting/','title':"FAQs",'section':"Docs",'content':"FAQs and Troubleshooting #  FAQs about INFINI Gateway and handling methods are provided here. You are welcome to submit your problems here.\nFAQs #  The Write Speed Is Not Improved #  Q: Why is the write speed not improved after I use bulk_reshuffle of the INFINI gateway?\nA: If your cluster has a small number of nodes, for example, if it contains less than 10 data nodes or if the index throughput is lower than 15w/s, you may not need to use this feature or you do not need to focus on the write performance because the cluster size is too small and forwarding and request distribution have a minimal impact on Elasticsearch. Therefore, the performance does not differ greatly regardless of whether the gateway is used. There are other benefits of using bulk_reshuffle, for example, the impact of faults occurring on the back-end Elasticsearch can be decoupled if data is sent to the gateway first.\nCommon Faults #  Port Reuse Is Not Supported #  Error prompt: The OS doesn\u0026rsquo;t support SO_REUSEPORT: cannot enable SO_REUSEPORT: protocol not available\nFault description: Port reuse is enabled on INFINI Gateway by default. It is used for multi-process port sharing. Patches need to be installed in the Linux kernel of the old version so that the port reuse becomes available.\nSolution: Modify the network monitoring configuration by changing reuse_port to false to disable port reuse.\n**. network: binding: 0.0.0.0:xx reuse_port: false An Elasticsearch User Does Not Have Sufficient Permissions #  Error prompt: [03-10 14:57:43] [ERR] [app.go:325] shutdown: json: cannot unmarshal object into Go value of type []adapter.CatIndexResponse\nFault description: After discovery is enabled for Elasticsearch on INFINI Gateway, this error is generated if the user permission is insufficient. The cause is that relevant Elasticsearch APIs need to be accessed to acquire cluster information.\nSolution: Grant the monitor and view_index_metadata permissions of all indexes to relevant Elasticsearch users.\n"});index.add({'id':21,'href':'/docs/getting-started/benchmark/','title':"Performance Test",'section':"Quick Start",'content':"Performance Test #  You are advised to use the Elasticsearch-dedicated benchmark tool Loadgen to test the gateway performance.\nHighlights of Loadgen:\n Robust performance Lightweight and dependency-free Random selection of template-based parameters High concurrency Balanced traffic control at the benchmark end   Download URL: http://release.elasticsearch.cn/loadgen/\n Loadgen #  Loadgen is easy to use. After the tool is downloaded and decompressed, two files are obtained: one executable program and one configuration file loadgen.yml. An example of the configuration file is as follows:\nvariables: - name: ip type: file path: test/ip.txt - name: user type: file path: test/user.txt - name: id type: sequence - name: uuid type: uuid - name: now_local type: now_local - name: now_utc type: now_utc - name: now_unix type: now_unix requests: - request: has_variable: true method: GET basic_auth: username: elastic password: pass url: http://localhost:8000/medcl/_search body: '{ \u0026quot;query\u0026quot;: {\u0026quot;match\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;$[[user]]\u0026quot; }}}' Use of Variables #  In the above configuration, variables is used to define variable parameters and variables are identified by name. In a constructed request, $[[Variable name]] can be used to access the value of the variable. Supported variable types are as follows:\n   Type Description     file External variable parameter of the file type   sequence Variable of the auto incremental numeric type   uuid Variable of the UUID character type   now_local Current time and local time zone   now_utc Current time and UTC time zone   now_unix Current time and Unix timestamp    Variable parameters of the file type are loaded from an external text file. One variable parameter occupies one line. When one variable of the file type is accessed, one variable value is taken randomly. An example of the variable format is as follows:\n➜ loadgen git:(master) ✗ cat test/user.txt medcl elastic Request Definition #  The requests node is used to set requests to be executed by Loadgen in sequence. Loadgen supports fixed-parameter requests (when has_variable is set to true) and requests constructed using template-based variable parameters. The following is an example of a common query request.\nrequests: - request: has_variable: true method: GET basic_auth: username: elastic password: pass url: http://localhost:8000/medcl/_search?q=name:$[[user]] In the above query, Loadgen conducts queries based on the medcl index and executes one query based on the name field. The value of each request is from the random variable user.\nCLI Parameters #  Loadgen cyclically executes requests defined in the configuration file. By default, Loadgen runs for 5s and then automatically exits. If you want to prolong the running time or increase the concurrency, you can set the tool\u0026rsquo;s startup parameters. The help commands are as follows:\n➜ loadgen git:(master) ✗ ./bin/loadgen --help Usage of ./bin/loadgen: -c int Number of concurrent threads (default 1) -compress Compress requests with gzip -config string the location of config file, default: loadgen.yml (default \u0026quot;loadgen.yml\u0026quot;) -cpu int the number of CPUs to use (default -1) -cpuprofile string write cpu profile to this file -d int Duration of tests in seconds (default 5) -daemon run in background as daemon -debug run in debug mode, loadgen will quit with panic error -l int Limit total requests (default -1) -log string the log level,options:trace,debug,info,warn,error (default \u0026quot;info\u0026quot;) -memprofile string write memory profile to this file -pidfile string pidfile path (only for daemon mode) -pprof string enable and setup pprof/expvar service, eg: localhost:6060 , the endpoint will be: http://localhost:6060/debug/pprof/ and http://localhost:6060/debug/vars -r int Max requests per second (fixed QPS) (default -1) -v\tversion Benchmark Test #  Run Loadgen to perform the benchmark test as follows:\n➜ loadgen git:(master) ✗ ./bin/loadgen -d 30 -c 100 -compress __ ___ _ ___ ___ __ __ / / /___\\/_\\ / \\/ _ \\ /__\\/\\ \\ \\ / / // ///_\\\\ / /\\ / /_\\//_\\ / \\/ / / /__/ \\_// _ \\/ /_// /_\\\\//__/ /\\ / \\____|___/\\_/ \\_/___,'\\____/\\__/\\_\\ \\/ [LOADGEN] A http load generator and testing suit. [LOADGEN] 1.0.0_SNAPSHOT, 83f2cb9, Sun Jul 4 13:52:42 2021 +0800, medcl, support single item in dict files [07-19 16:15:00] [INF] [instance.go:24] workspace: data/loadgen/nodes/0 [07-19 16:15:00] [INF] [loader.go:312] warmup started [07-19 16:15:00] [INF] [app.go:306] loadgen now started. [07-19 16:15:00] [INF] [loader.go:316] [GET] http://localhost:8000/medcl/_search [07-19 16:15:00] [INF] [loader.go:317] status: 200,\u0026lt;nil\u0026gt;,{\u0026quot;took\u0026quot;:1,\u0026quot;timed_out\u0026quot;:false,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;successful\u0026quot;:1,\u0026quot;skipped\u0026quot;:0,\u0026quot;failed\u0026quot;:0},\u0026quot;hits\u0026quot;:{\u0026quot;total\u0026quot;:{\u0026quot;value\u0026quot;:0,\u0026quot;relation\u0026quot;:\u0026quot;eq\u0026quot;},\u0026quot;max_score\u0026quot;:null,\u0026quot;hits\u0026quot;:[]}} [07-19 16:15:00] [INF] [loader.go:316] [GET] http://localhost:8000/medcl/_search?q=name:medcl [07-19 16:15:00] [INF] [loader.go:317] status: 200,\u0026lt;nil\u0026gt;,{\u0026quot;took\u0026quot;:1,\u0026quot;timed_out\u0026quot;:false,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;successful\u0026quot;:1,\u0026quot;skipped\u0026quot;:0,\u0026quot;failed\u0026quot;:0},\u0026quot;hits\u0026quot;:{\u0026quot;total\u0026quot;:{\u0026quot;value\u0026quot;:0,\u0026quot;relation\u0026quot;:\u0026quot;eq\u0026quot;},\u0026quot;max_score\u0026quot;:null,\u0026quot;hits\u0026quot;:[]}} [07-19 16:15:01] [INF] [loader.go:316] [POST] http://localhost:8000/_bulk [07-19 16:15:01] [INF] [loader.go:317] status: 200,\u0026lt;nil\u0026gt;,{\u0026quot;took\u0026quot;:120,\u0026quot;errors\u0026quot;:false,\u0026quot;items\u0026quot;:[{\u0026quot;index\u0026quot;:{\u0026quot;_index\u0026quot;:\u0026quot;medcl-y4\u0026quot;,\u0026quot;_type\u0026quot;:\u0026quot;doc\u0026quot;,\u0026quot;_id\u0026quot;:\u0026quot;c3qj9123r0okahraiej0\u0026quot;,\u0026quot;_version\u0026quot;:1,\u0026quot;result\u0026quot;:\u0026quot;created\u0026quot;,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:2,\u0026quot;successful\u0026quot;:1,\u0026quot;failed\u0026quot;:0},\u0026quot;_seq_no\u0026quot;:5735852,\u0026quot;_primary_term\u0026quot;:3,\u0026quot;status\u0026quot;:201}}]} [07-19 16:15:01] [INF] [loader.go:325] warmup finished 5253 requests in 32.756483336s, 524.61KB sent, 2.49MB received [Loadgen Client Metrics] Requests/sec:\t175.10 Request Traffic/sec:\t17.49KB Total Transfer/sec:\t102.34KB Avg Req Time:\t5.711022ms Fastest Request:\t440.448µs Slowest Request:\t3.624302658s Number of Errors:\t0 Number of Invalid:\t0 Status 200:\t5253 [Estimated Server Metrics] Requests/sec:\t160.37 Transfer/sec:\t93.73KB Avg Req Time:\t623.576686ms Loadgen executes all requests once to warm up before the formal benchmark test. If an error occurs, a prompt is displayed, asking you whether to continue. The warm-up request results are also output to the terminal. After execution, an execution summary is output.\n The final results of Loadgen are the cumulative statistics after all requests are executed, and they may be inaccurate. You are advised to start the Kibana dashboard to check all operating indicators of Elasticsearch in real time.\n Simulating Bulk Ingestion #  It is very easy to use Loadgen to simulate bulk ingestion. Configure one index operation in the request body and then use the body_repeat_times parameter to randomly replicate several parameterized requests to complete the preparation of a batch of requests. See the following example.\n - request: method: POST has_variable: true basic_auth: username: test password: testtest url: http://localhost:8000/_bulk body_repeat_times: 1000 body: \u0026quot;{ \\\u0026quot;index\\\u0026quot; : { \\\u0026quot;_index\\\u0026quot; : \\\u0026quot;medcl-y4\\\u0026quot;,\\\u0026quot;_type\\\u0026quot;:\\\u0026quot;doc\\\u0026quot;, \\\u0026quot;_id\\\u0026quot; : \\\u0026quot;$[[uuid]]\\\u0026quot; } }\\n{ \\\u0026quot;id\\\u0026quot; : \\\u0026quot;$[[id]]\\\u0026quot;,\\\u0026quot;field1\\\u0026quot; : \\\u0026quot;$[[user]]\\\u0026quot;,\\\u0026quot;ip\\\u0026quot; : \\\u0026quot;$[[ip]]\\\u0026quot;,\\\u0026quot;now_local\\\u0026quot; : \\\u0026quot;$[[now_local]]\\\u0026quot;,\\\u0026quot;now_unix\\\u0026quot; : \\\u0026quot;$[[now_unix]]\\\u0026quot; }\\n\u0026quot; Limiting the Client Workload #  You can use Loadgen and set the CLI parameter -r to restrict the number of requests that can be sent by the client per second, so as to evaluate the response time and load of Elasticsearch under fixed pressure. See the following example.\n➜ loadgen git:(master) ✗ ./bin/loadgen -d 30 -c 100 -r 100  Note: The client throughput limit may not be accurate enough in the case of massive concurrencies.\n Limiting the Total Number of Requests #  You can set the -l parameter to control the total number of requests that can be sent by the client, so as to generate a fixed number of documents. Modify the configuration as follows:\nrequests: - request: method: POST has_variable: true basic_auth: username: test password: testtest url: http://localhost:8000/medcl-test/doc2/_bulk body_repeat_times: 1 body: \u0026quot;{ \\\u0026quot;index\\\u0026quot; : { \\\u0026quot;_index\\\u0026quot; : \\\u0026quot;medcl-test\\\u0026quot;, \\\u0026quot;_id\\\u0026quot; : \\\u0026quot;$[[uuid]]\\\u0026quot; } }\\n{ \\\u0026quot;id\\\u0026quot; : \\\u0026quot;$[[id]]\\\u0026quot;,\\\u0026quot;field1\\\u0026quot; : \\\u0026quot;$[[user]]\\\u0026quot;,\\\u0026quot;ip\\\u0026quot; : \\\u0026quot;$[[ip]]\\\u0026quot; }\\n\u0026quot; Configured parameters use the content of only one document for each request. Then, the system executes Loadgen.\n./bin/loadgen -config loadgen-gw.yml -d 600 -c 100 -l 50000 After execution, 50000 records are added for the Elasticsearch index medcl-test.\nUsing Auto Incremental IDs to Ensure the Document Sequence #  If the IDs of generated documents need to increase regularly to facilitate comparison, you can use the auto incremental IDs of the sequence type as the primary key and avoid using random numbers in the content. See the following example.\nrequests: - request: method: POST has_variable: true basic_auth: username: test password: testtest url: http://localhost:8000/medcl-test/doc2/_bulk body_repeat_times: 1 body: \u0026quot;{ \\\u0026quot;index\\\u0026quot; : { \\\u0026quot;_index\\\u0026quot; : \\\u0026quot;medcl-test\\\u0026quot;, \\\u0026quot;_id\\\u0026quot; : \\\u0026quot;$[[id]]\\\u0026quot; } }\\n{ \\\u0026quot;id\\\u0026quot; : \\\u0026quot;$[[id]]\\\u0026quot; }\\n\u0026quot; "});index.add({'id':22,'href':'/docs/references/filters/','title':"Online Filter",'section':"Reference",'content':"Request Filter #  What Is a Filter #  A filter is a series of processing units defined in a flow for requests received by the gateway. Each filter processes one task and the filters can be flexibly combined. Filters process requests online.\nFilter List #  Request Filtering #    context_filter  request_method_filter  request_header_filter  request_path_filter  request_user_filter  request_host_filter  request_client_ip_filter  request_api_key_filter  response_status_filter  response_header_filter  Request Forwarding #    ratio  clone  switch  flow  Request Mutation #    sample  request_body_json_del  request_body_json_set  context_regex_replace  request_body_regex_replace  response_body_regex_replace  response_header_format  set_context  set_basic_auth  set_hostname  set_request_header  set_request_query_args  set_response_header  set_response  Traffic Control and Throttling #    context_limiter  request_path_limiter  request_host_limiter  request_user_limiter  request_api_key_limiter  request_client_ip_limiter  retry_limiter  sleep  Log Monitoring #    logging  Elasticsearch #    date_range_precision_tuning  bulk_reshuffle  elasticsearch_health_check  bulk_response_process  bulk_request_mutate  Authentication #    basic_auth  ldap_auth  Output #    queue  elasticsearch  cache  translog  redis_pubsub  drop  http  Debugging and Development #    echo  dump  record  "});index.add({'id':23,'href':'/docs/release-notes/','title':"Release Notes",'section':"Docs",'content':"Release Notes #  Information about release notes of INFINI Gateway is provided here.\n1.6.0 #  Breaking changes #   Update disk_queue folder structure, use UUID as folder name instead of the queue name Parameter mode was removed from bulk_reshuffle filter, only async was supported Rename filter bulk_response_validate to bulk_response_process  Features #   Add metadata to queue Support subscribe queue by specify labels Support concurrent worker control for bulk_indexing processor Auto detect new queues for bulk_indexing processor Allow to consume queue messages over disk queue Auto sync disk_queue files to remote s3 in background Add api to operate gateway entry Support plugin auto discovery Add API to operate gateway entities Filter bulk_request_mutate support remove _type in bulk requests for es v8.0+ Add elasticsearch adapter for version 8.0+ Add http filter for general reverse proxy usage, like proxy Kibana Add consumer_has_lag condition to check queue status Add record filter to play requests easier Add zstd compress to disk_queue, disabled by default  Bug fix #   Fix date_range_precision_tuning filter for complex range query Fix node availability initially check Fix basic_auth filter not asking user to input auth info in browser  Improvements #   Handle http public address, remove prefix if that exists Refactor bulk_reshuffle filter and bulk_indexing processor Should not fetch nodes info when elasticsearch discovery disabled Seamless consume queue message across files Persist consumer offset to local store Add API to reset consumer offset Refactoring ORM framework Expose error of mapping put Refactoring pipeline framework Improve multi-instance check, multi-instance disabled by default Add CPU and memory metrics to stats api Seamless fetch queue files from s3 server Proper handle 409 version conflicts in bulk requests  1.5.0 #  Breaking changes #  Features #   Add API to scroll messages from disk queue Prevent out of space, disk usage reserved for disk_queue Add context_filter and context_limiter for general purpose Add bulk_request_mutate filter Add basic_auth filter Add set_context filter Add context_regex_replace filter Add to_string property to request and response context  Bug fix #   Fix bulk response validate incorrectly caused by jsonParser Fix nil exception in request_path_limiter caused by refactoring Fix big size document out of order caused by bulk buffer  Improvements #   Fix TCP not keepalived in some case Add closing progress bar to pipeline module Add retry_delay_in_ms config to pipeline module Handle partial failure in bulk requests Optimize scroll performance of dump_hash processor Improve API directory  1.4.0 #  Breaking changes #   Rename flow config filter_v2 to filter, only support new syntax Rename pipeline config pipelines_v2 to pipeline, processors to processor, only support new syntax Rename filter request_logging to logging Merge dump filters to dump filter Response headers renamed, dashboard may broken Remove filter request_body_truncate and response_body_truncate  Features #   Add option to disable file logging output Add option compress to queue_consumer processor  Bug fix #   Fix invalid host header setting in elasticsearch reverse proxy Fix cluster available health check Fix gzip encoding issue for requests forwarding  Improvements #   Support string type in in condition  1.3.0 #  Breaking changes #   Switch to use pipelines_v2 syntax only Rename filter disk_enqueue to queue Rename processor disk_queue_consumer to queue_consumer Rename filter redis to redis_pubsub  Features #   Refactoring pipeline framework, support DAG based task schedule Add dump_hash and index_diffs processor Add redis output and redis queue adapter Add set_request_query_args filter Add ldap_auth filter Add retry_limiter filter Add request_body_json_set and request_body_json_del filter Add stats filter Add health_check config to elastic module Add API to pipeline framework, support _start and _stop pipelines  Bug fix #   Fix data race issue in bulk_reshuffle Fix fix_null_id always executed in bulk_reshuffle Auto handle big sized documents in bulk requests  Improvements #   Refactoring flow runner to service pipeline Optimize CPU and Memory usage Optimize index diff service, speedup and cross version compatibility Set the default max file size of queue files to 1 GB Proper handle elasticsearch failure during startup Support custom depth check to queue_has_lag condition Support multi hosts for elasticsearch configuration Add parameter auto_start to prevent pipeline running on start Add keep_running parameter to pipeline config Safety shutdown pipeline and entry service Support more complex routing pattern rules  1.2.0 #  Features #   Support alias in bulk_reshuffle filter. Support truncate in request_logging filter. Handle 429 retry in json_indexing service. Add forcemerge service. Add response_body_regex_replace filter. Add request_body_regex_replace filter. Add sleep filter. Add option to log slow requests only. Add cluster and bulk status to request logging. Add filter_v2 and support _ctx to access request context. Add dump_context filter. Add translog filter, support rotation and compression. Add set_response filter. Add set_request_header filter. Add set_hostname filter. Add set_basic_auth filter. Add set_response_header filter. Add elasticsearch_health_check filter. Add drop filter.  Bug fix #   Fix truncate body filter, correctly resize the body bytes. Fix cache filter. Fix floating_ip module. Fix dirty write in diskqueue. Fix compression enabled requests. Fix date_range_precision_tuning filter. Fix invalid indices status on closed indices #23. Fix document hash for elasticsearch 6.x. Fix floating_ip feature run with daemon mode. Fix async bulk to work with beats.  Improvements #   Optimize memory usage, fix memory leak.  Acknowledgement #  Thanks to the following enterprises and teams #   China Everbright Bank, China Citic Bank, BSG, Yogoo  Thanks to the following individual contributors #   MaQianghua, YangFan, Tanzi, FangLi  1.1.0 #   Request Logging and Dashboard. Support ARM Platform [armv5\\v6\\v7\\v8(arm64)]. Fix Elasticsearch Nodes Auto Discovery. Add Request Header Filter. Add Request Method Filter. Add Sample Filter. Request Logging Performance Optimized (100x speedup). Add Request Path Filter. Add Debug Filter. Add User Info to Logging Message. Support Routing Partial Traffic to Specify Processing Flow (by Ratio). Support Traffic Clone, Support Dual-Write or 1:N Write. Elasticsearch topology auto discovery, support filter by nodes,tags,roles. Backend failure auto detection, auto retry and select another available endpoint. Floating IP feature ready to use. Add bulk_reshuffle filter.  1.0.0 #   Rewritten for performance Index level request throttle Request caching Kibana MAGIC speedup Upstream auto discovery Weighted upstream selections Max connection limit per upstream  "});index.add({'id':24,'href':'/docs/references/processors/','title':"Offline Processor",'section':"Reference",'content':"Pipeline #  What Is Pipeline? #  A pipeline is a function combination used for processing tasks offline. It uses the pipeline design pattern, just as online request filters do. A processor is the basic unit of a pipeline. Each processing component focuses on one task and the components can be flexibly assembled, and plugged and removed as required.\nPipeline Definition #  A typical pipeline service is defined as follows:\npipeline: - name: request_logging_index auto_start: true keep_running: true processor: - json_indexing: index_name: \u0026quot;gateway_requests\u0026quot; elasticsearch: \u0026quot;dev\u0026quot; input_queue: \u0026quot;request_logging\u0026quot; idle_timeout_in_seconds: 1 worker_size: 1 bulk_size_in_mb: 10 #in MB In the above configuration, a processing pipeline named request_logging_index is defined, and the processor parameter defines several processing units for the pipeline, which are executed in sequence.\nParameter Description #  Parameters related to pipeline definition are described as follows:\n   Name Type Description     name string Name of a pipeline, which must be unique   auto_start bool Whether the pipeline automatically starts with the gateway startup, that is, whether the task is executed immediately   keep_running bool Whether the gateway starts executing the task again after completing the execution   retry_delay_in_ms int Minimum waiting time for the task re-execution, which is set to 5000 milliseconds by default   processor array List of processors to be executed by the pipeline in sequence    Processor List #  Task Scheduling #    dag  Index Writing #    bulk_indexing  json_indexing  queue_consumer  Index Contrast #    dump_hash  index_diff  Request Processing #    flow_runner  Request Replay #    replay  "});index.add({'id':25,'href':'/docs/tutorial/proxy_kibana/','title':"Adding a Proxy and Basic Security for Kibana",'section':"Tutorials",'content':"Adding a Proxy and Basic Security for Kibana #  If you have multiple Kibana versions or your version is out of date, or if you do not set TLS or identity, then anyone can directly access Kibana. You can use INFINI Gateway to quickly fix this issue.\nUsing the HTTP Filter to Forward Requests #   - http: schema: \u0026quot;http\u0026quot; #https or http host: \u0026quot;192.168.3.188:5602\u0026quot; Adding Authentication #   - basic_auth: valid_users: medcl: passwd Replacing Static Resources in the Router #   - method: - GET pattern: - \u0026quot;/plugins/kibanaReact/assets/illustration_integrations_lightmode.svg\u0026quot; flow: - replace_logo_flow Enabling TLS #   - name: my_es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8000 skip_occupied_port: true tls: enabled: true Complete Configuration #  entry: - name: my_es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8000 skip_occupied_port: true tls: enabled: true flow: - name: logout_flow filter: - set_response: status: 401 body: \u0026quot;Success logout!\u0026quot; - drop: - name: replace_logo_flow filter: - redirect: uri: https://elasticsearch.cn/uploads/event/20211120/458c74ca3169260dbb2308dd06ef930a.png - name: default_flow filter: - basic_auth: valid_users: medcl: passwd - http: schema: \u0026quot;http\u0026quot; #https or http host: \u0026quot;192.168.3.188:5602\u0026quot; router: - name: my_router default_flow: default_flow rules: - method: - GET - POST pattern: - \u0026quot;/_logout\u0026quot; flow: - logout_flow - method: - GET pattern: - \u0026quot;/plugins/kibanaReact/assets/illustration_integrations_lightmode.svg\u0026quot; flow: - replace_logo_flow Effect #  To access Kibana through INFINI Gateway, you need to log in as follows:\nAfter login, you will find that resources in Kibana are also replaced. See the figure below.\nProspect #  We can explore other benefits of INFINI Gateway, for example, we can use INFINI Gateway to replace the logo, js, and CSS style in Kibana, or use the combination of js and CSS to dynamically add navigation and pages and achieve visualization.\n"});index.add({'id':26,'href':'/docs/tutorial/fix_count_in_search_response/','title':"Compatible with the Count Structure of Query Response Results of Different Elasticsearch Versions",'section':"Tutorials",'content':"Compatible with the Count Structure of Query Response Results of Different Elasticsearch Versions #  To optimize performance in Elasticsearch 7.0 and later versions, search result matches are not accurately counted and the search result response body is adjusted. This will inevitably cause incompatibility with existing code. How can the problem be fixed quickly?\nStructure Contrast #  The search structure difference is as follows:\nThe search structure used by Elasticsearch before version 7.0 is as follows. total shows a specific value.\n{ \u0026quot;took\u0026quot;: 53, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: 0, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] } } The search structure used by Elasticsearch 7.0 and later versions is as follows. total shows a group of description scope objects.\n{ \u0026quot;took\u0026quot;: 3, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: 1, \u0026quot;hits\u0026quot;: [] } } Parameters Provided by Elasticsearch #  Elasticsearch 7.0 provides a parameter to accurately control the count. In other words, rest_total_hits_as_int=true can be added to the query request URL parameter so that the old structure is used. It is disabled by default.\nDocument URL: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html\nHowever, you need to modify the program to add this parameter, and you may need to adjust the back-end code, front-end paging, and presentation. The modification workload may not be small.\nUsing INFINI Gateway for Quick Fixing #  If you do not want to modify the program, you can use INFINI Gateway to quickly repair the query and add query parameters to a search query. In addition, INFINI Gateway can be used to limit the request sources for which query parameters are to be added. For example, request sources can be adjusted only for specific service calling parties. The following uses the curl command as an example to add query parameters only to queries from the curl debugging.\nentry: - name: es_entrypoint enabled: true router: default network: binding: 0.0.0.0:8000 router: - name: default default_flow: main_flow flow: - name: main_flow filter: - set_request_query_args: args: - rest_total_hits_as_int -\u0026gt; true when: and: - contains: _ctx.request.path: \u0026quot;_search\u0026quot; - equals: _ctx.request.header.User-Agent: \u0026quot;curl/7.54.0\u0026quot; - record: stdout: true - elasticsearch: elasticsearch: es-server - dump: response_body: true elasticsearch: - name: es-server enabled: true endpoints: - http://192.168.3.188:9206 The final effect is as follows:\nFigure 1 shows the search result returned after the gateway is accessed through a browser. Figure 2 shows the search result returned by the curl command. The User-Agent header information can match the curl command and only parameters are added to the search conditions to avoid affecting other requests.\n"});index.add({'id':27,'href':'/docs/tutorial/es-hadoop_integration/','title':"Integration with Elasticsearch-Hadoop",'section':"Tutorials",'content':"Integration with Elasticsearch-Hadoop #  Elasticsearch-Hadoop utilizes a seed node to access all back-end Elasticsearch nodes by default. The hotspots and requests may be improperly allocated. To improve the resource utilization of back-end Elasticsearch nodes, you can implement precision routing for the access to Elasticsearch nodes through INFINI Gateway.\nWrite Acceleration #  If you import data by using Elasticsearch-Hadoop, you can modify the following parameters of Elasticsearch-Hadoop to access INFINI Gateway, so as to improve the write throughput:\n   Name Type Description     es.nodes string List of addresses used to access the gateway, for example, localhost:8000,localhost:8001   es.nodes.discovery bool When it is set to false, the sniff mode is not adopted and only the configured back-end nodes are accessed.   es.nodes.wan.only bool When it is set to true, it indicates the proxy mode, in which data is forcibly sent through the gateway address.   es.batch.size.entries int Batch document quantity. Set the parameter to a larger value to improve throughput, for example, 5000.   es.batch.size.bytes string Batch transmission size. Set the parameter to a larger value to improve throughput, for example, 20mb.   es.batch.write.refresh bool Set it to false to prevent active refresh and improve throughput.    Related Link #    Elasticsearch-Hadoop Configuration Parameter Document  "});index.add({'id':28,'href':'/docs/resources/','title':"Other Resources",'section':"Docs",'content':"Other Resources #  Useful external resources related to INFINI Gateway are provided here.\nProject #    Elasticsearch INFINI Gateway on Kubernetes HA (HA Solution)  Articles #    Elasticsearch INFINI Gateway Test Version Release  INFINI Gateway First Experience  INFINI Gateway Use Method and Use Experience  Extraordinary Performance  INFINI Gateway Performance and Pressure Test Results  Quadruple Indexing Speed Improvement  Elasticsearch Cross-Cluster Cross-Version Query and All Other Requests by Using INFINI Gateway  Elasticsearch DR Technology Discussion and Testing  Solving the Upgrade of the Elasticsearch Cluster Architecture by Using INFINI Gateway  Processing the Apache Log4j Vulnerability of Elasticsearch by Using INFINI Gateway  Videos #    What Is INFINI Gateway  Introduction to Downloading and Installation of INFINI Gateway  How to Install INFINI Gateway as a System Service  INFINI Gateway Configuration Description  Elasticsearch Remote DR Solution  "});index.add({'id':29,'href':'/docs/references/config/','title':"Other Configuration",'section':"Reference",'content':"Other Configuration #  System Configuration #  System configuration is used to set the configurations of INFINI Gateway.\n   Name Type Description     path.data string Data directory, which is data by default.   path.logs string Log directory, which is log by default.   path.configs string Configuration directory, which is config by default.   log.level string Log level, which is info by default.   log.debug bool Whether to enable the debugging mode, which is false by default. After the debugging mode is enabled, if an exception occurs, the program exits directly and prints the complete stack, which is only used for debugging and locating faults. Do not enable it in the production environment because data may be lost.   log.disable_file_output bool Whether to disable the local log file output, which is set to false by default. Enable this parameter if local log output is not required in the container environment.   allow_multi_instance bool Whether to start multiple gateway instances on a single machine, which is set to false by default.   max_num_of_instances int Maximum number of gateway instances, which is set to 5 by default.   configs_auto_reload bool Whether dynamic loading configured in path.configs is supported    Configuring the Local Disk Array #  Example:\ndisk_queue: upload_to_s3: true s3: server: my_blob_store location: cn-beijing-001 bucket: infini-store max_bytes_per_file: 102400    Name Type Description     disk_queue.min_msg_size int Minimum limit on the bytes of a single message sent to a queue. The default value is 1.   disk_queue.max_msg_size int Maximum limit on the bytes of a single message sent to a queue. The default value is 104857600, that is, 100 MB.   disk_queue.sync_every_records int Number of records, after which one disk synchronization is performed. The default value is 1000.   disk_queue.sync_timeout_in_ms int Interval for performing one disk synchronization operation. The default value is 1000 ms.   disk_queue.max_bytes_per_file int Maximum size of a single file in the local disk queue. When the size of a file exceeds this value, a new file is automatically generated. The default value is 104857600, that is, 100 MB.   disk_queue.max_used_bytes int Maximum allowable storage space in the local disk queue.   disk_queue.warning_free_bytes int Idle storage space when the disk usage reaches the alarm threshold. The default value is 10737418240, that is, 10 GB.   disk_queue.reserved_free_bytes int Protection value of the idle storage space of the disk. The disk becomes read-only when this value is reached. The default value is 5368709120, that is, 5 GB.   disk_queue.upload_to_s3 bool Whether to upload disk queue files to S3. The default value is false.   disk_queue.s3.async bool Whether to asynchronously upload files to S3   disk_queue.s3.server string ID of the S3 server   disk_queue.s3.location string Location of the S3 server   disk_queue.s3.bucket string Bucket of the S3 server   disk_queue.retention.max_num_of_local_files int Maximum number of latest files sorted by creation time that can be kept on the local disk after files are uploaded to the S3 server. The default value is 10.    Configuring the S3 Server Resources #  Example:\ns3: my_blob_store: endpoint: \u0026quot;192.168.3.188:9000\u0026quot; access_key: \u0026quot;admin\u0026quot; access_secret: \u0026quot;gogoaminio\u0026quot;    Name Type Description     s3.[id].endpoint string Address of the S3 server   s3.[id].access_key string Key of the S3 server   s3.[id].access_secret string Secret key of the S3 server   s3.[id].token string Token information about the S3 server   s3.[id].ssl bool Whether TLS is enabled on the S3 server    "});index.add({'id':30,'href':'/docs/references/filters/basic_auth/','title':"basic_auth",'section':"Online Filter",'content':"basic_auth #  Description #  The basic_auth filter is used to verify authentication information of requests. It is applicable to simple authentication.\nConfiguration Example #  A simple example is as follows:\nflow: - name: basic_auth filter: - basic_auth: valid_users: medcl: passwd medcl1: abc ... Parameter Description #     Name Type Description     valid_users map Username and password    "});index.add({'id':31,'href':'/docs/references/processors/bulk_indexing/','title':"bulk_indexing",'section':"Offline Processor",'content':"bulk_indexing #  Description #  The bulk_indexing processor is used to asynchronously consume bulk requests in queues.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - bulk_indexing: bulk_size_in_mb: 1 queues: type: bulk_reshuffle level: cluster Parameter Description #     Name Type Description     idle_timeout_in_seconds int Timeout duration of the consumption queue, which is set to 1 by default.   max_connection_per_node int Maximum number of connections allowed by the target node. The default value is 1.   bulk_size_in_kb int Size of a bulk request, in KB.   bulk_size_in_mb int Size of a bulk request, in MB.   queues map A group of queues filtered by label, in which data needs to be consumed.   skip_info_missing bool Whether to ignore queue data consumption when conditions are not met, for example, the node, index, or shard information does not exist, that is, whether to consume queue data after information is obtained. The default value is false. Otherwise, one Elasticsearch node is selected to send requests.   bulk.compress bool Whether to enable request compression.   bulk.retry_delay_in_seconds int Waiting time for request retry.   bulk.reject_retry_delay_in_seconds int Waiting time for request rejection.   bulk.max_retry_times int Maximum retry count.   bulk.failure_queue string Queue for storing requests that fail because of a back-end failure.   bulk.invalid_queue string Queue for storing requests, for which 4xx is returned because of invalid requests.   bulk.dead_letter_queue string Request queue, for which the maximum retry count is exceeded.   bulk.safety_parse bool Whether to enable secure parsing, that is, no buffer is used and memory usage is higher. The default value is true.   bulk.doc_buffer_size bool Maximum document buffer size for the processing of a single request. You are advised to set it to be greater than the maximum size of a single document. The default value is 256*1024.    "});index.add({'id':32,'href':'/docs/references/filters/bulk_request_mutate/','title':"bulk_request_mutate",'section':"Online Filter",'content':"bulk_request_mutate #  Description #  The bulk_request_mutate filter is used to mutate bulk requests of Elasticsearch.\nConfiguration Example #  A simple example is as follows:\nflow: - name: bulk_request_mutate filter: - bulk_request_mutate: fix_null_id: true generate_enhanced_id: true # fix_null_type: true # default_type: m-type # default_index: m-index # index_rename: # \u0026quot;*\u0026quot;: index-new # index1: index-new # index2: index-new # index3: index3-new # index4: index3-new # medcl-dr3: index3-new # type_rename: # \u0026quot;*\u0026quot;: type-new # type1: type-new # type2: type-new # doc: type-new # doc1: type-new ... Parameter Description #     Name Type Description     fix_null_type bool Whether to fix a request that does not carry _type. It is used in collaboration with the default_type parameter.   fix_null_id bool Whether to fix a request that does not carry _id and generate a random ID, for example, c616rhkgq9s7q1h89ig0   remove_type bool Whether to remove the _type parameter. Elasticsearch versions higher than 8.0 do not support the _type parameter.   generate_enhanced_id bool Whether to generate an enhanced ID, such as c616rhkgq9s7q1h89ig0-1635937734071093-10.   default_index string Default index name, which is used if no index name is specified in metadata   default_type string Default document type, which is used if no document type is specified in metadata   index_rename map Index name used for renaming. You can use * to overwrite all index names.   type_rename map Type used for renaming. You can use * to overwrite all type names.   pipeline string pipeline parameter of a specified bulk request   remove_pipeline bool Whether to remove the pipeline parameter from the bulk request   safety_parse bool Whether to use a secure bulk metadata parsing method. The default value is true.   doc_buffer_size int Buffer size when an insecure bulk metadata parsing method is adopted. The default value is 256 * 1024.    "});index.add({'id':33,'href':'/docs/references/filters/bulk_reshuffle/','title':"bulk_reshuffle",'section':"Online Filter",'content':"bulk_reshuffle #  Description #  The bulk_reshuffle filter is used to parse batch requests of Elasticsearch based on document, sort out documents as needed, and archive and store them in queues. After documents are stored, the filter can rapidly return service requests, thereby decoupling front-end writing from back-end Elasticsearch clusters. The bulk_reshuffle filter needs to be used in combination with offline pipeline consumption tasks.\nWhen passing through queues generated by the bulk_reshuffle filter, metadata carries \u0026quot;type\u0026quot;: \u0026quot;bulk_reshuffle\u0026quot; and Elasticsearch cluster information such as \u0026quot;elasticsearch\u0026quot;: \u0026quot;dev\u0026quot;, by default. You can call APIs on the gateway to check metadata defined in queues. See the following example.\ncurl http://localhost:2900/queue/stats { \u0026quot;queue\u0026quot;: { \u0026quot;disk\u0026quot;: { \u0026quot;async_bulk-cluster##dev\u0026quot;: { \u0026quot;depth\u0026quot;: 0, \u0026quot;metadata\u0026quot;: { \u0026quot;source\u0026quot;: \u0026quot;dynamic\u0026quot;, \u0026quot;id\u0026quot;: \u0026quot;c71f7pqi4h92kki4qrvg\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;async_bulk-cluster##dev\u0026quot;, \u0026quot;label\u0026quot;: { \u0026quot;elasticsearch\u0026quot;: \u0026quot;dev\u0026quot;, \u0026quot;level\u0026quot;: \u0026quot;cluster\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;bulk_reshuffle\u0026quot; } } } } } } Node-Level Asynchronous Submission #  INFINI Gateway is capable of locally calculating the target storage location of a back-end Elasticsearch cluster corresponding to each index document so as to precisely locate requests. A batch of bulk requests may contain the data of multiple back-end nodes. The bulk_reshuffle filter is used to shuffle normal bulk requests and reassemble them based on target nodes or shards. The purpose is to prevent Elasticsearch nodes from distributing received requests, so as to reduce the traffic and load between Elasticsearch clusters. The filter also prevents a single node from becoming a bottleneck and ensures balanced processing of all data nodes, thereby improving the overall index throughput of clusters.\nDefining a Flow #  A simple example is as follows:\nflow: - name: online_indexing_merge filter: - bulk_reshuffle: elasticsearch: prod level: node #cluster,node,shard,partition - elasticsearch: elasticsearch: prod refresh: enabled: true interval: 30s The above configuration indicates that bulk requests will be split and reassembled based on the target nodes corresponding to index documents. Data is sent to local disk queues first and then consumed and submitted through separate tasks to the target Elasticsearch nodes.\nThe benefit of this filter is that a failure occurring on the back-end Elasticsearch cluster will not affect indexing operations because requests are stored in disk queues of the gateway and the front-end indexing is decoupled from back-end clusters. Therefore, when the back-end Elasticsearch cluster encounters a failure, restarts, or initiates version upgrade, normal index operations will not be affected.  Configuring a Consumption Pipeline #  After the gateway sends requests to the disk, a consumption queue pipeline needs to be configured as follows to submit data:\npipeline: - name: bulk_request_ingest auto_start: true processor: - bulk_indexing: bulk_size_in_mb: 10 #in MB queues: type: bulk_reshuffle level: node One pipeline task named bulk_request_ingest is used and the filter conditions for queues of to-be-subscribed targets are type: bulk_reshuffle and level: node. You can also set the batch size for bulk submission. In this way, node-level requests received by INFINI Gateway will be automatically sent to the corresponding Elasticsearch node.\nShard-Level Asynchronous Submission #  Shard-level asynchronous submission is suitable for scenarios in which the data amount of a single index is large and needs to be processed independently. An index is split into shards and then bulk requests are submitted in the form of shards, which further improves the processing efficiency of back-end Elasticsearch nodes.\nThe configuration is as follows:\nDefining a Flow #  flow: - name: online_indexing_merge filter: - bulk_reshuffle: elasticsearch: prod level: shard - elasticsearch: elasticsearch: prod refresh: enabled: true interval: 30s Set the assembly and disassembly level to the shard type.\nDefining a Pipeline #  pipeline: - name: bulk_request_ingest auto_start: true processor: - bulk_indexing: queues: type: bulk_reshuffle level: shard bulk_size_in_mb: 1 #in MB Compared with the preceding node-level configuration, the level parameter is modified to listen to shard-type disk queues. If there are many indexes, excess local disk queues will cause extra overhead. You are advised to enable this mode only for specific indexes whose throughput needs to be optimized.\nParameter Description #     Name Type Description     elasticsearch string Name of an Elasticsearch cluster instance.   level string Shuffle level of a request, that is, cluster level. The default value is cluster. It can be set to cluster, node, index, or shard.   partition_size int Maximum partition size. Partitioning is performed by document _id on the basis of level.   fix_null_id bool Whether to automatically generate a random UUID if no document ID is specified in the bulk index request document. It is applicable to data of the log type. The default value is true.   index_stats_analysis bool Whether to record index name statistics to request logs. The default value is true.   action_stats_analysis bool Whether to record bulk request statistics to request logs. The default value is true.   doc_buffer_size int Buffer size of a processing document. If a single index document is very large, the value needs to be greater than the document size. The default value is 262144, or, 256 KB.   shards array Index shards that can be processed. The value is a character array, for example, \u0026quot;0\u0026quot;. All shards are processed by default, and you can set specific shards to be processed.   tag_on_success array Specified tag to be attached to request context after all bulk requests are processed.    "});index.add({'id':34,'href':'/docs/references/filters/bulk_response_process/','title':"bulk_response_process",'section':"Online Filter",'content':"bulk_response_process #  Description #  The bulk_response_process filter is used to process bulk requests of Elasticsearch.\nConfiguration Example #  A simple example is as follows:\nflow: - name: bulk_response_process filter: - bulk_response_process: success_queue: \u0026quot;success_queue\u0026quot; tag_on_success: [\u0026quot;commit_message_allowed\u0026quot;] Parameter Description #     Name Type Description     invalid_queue string Name of the queue that saves an invalid request. It is mandatory.   failure_queue string Name of the queue that saves a failed request. It is mandatory.   save_partial_success_requests bool Whether to save partially successful requests in bulk requests. The default value is false.   success_queue string Queue that saves partially successful requests in the bulk requests   doc_buffer_size int Size of the buffer for processing a single document. The default value is 25 KB, or, 262144.   continue_on_error bool Whether to continue to execute subsequent filters after an error occurs on a bulk request. The default value is false.   message_truncate_size int Truncation length of a bulk request error log. The default value is 1024.   safety_parse bool Whether to use a secure bulk metadata parsing method. The default value is true.   doc_buffer_size int Buffer size when an insecure bulk metadata parsing method is adopted. The default value is 256 * 1024.   tag_on_success array Specified tag to be attached to request context after all bulk requests are processed.   tag_on_error array Specified tag to be attached to request context after an error occurs on a request.   tag_on_partial array Specified tag to be attached to request context after requests in a bulk request are partially executed successfully.   tag_on_failure array Specified tag to be attached to request context after some requests in a bulk request fail (retry is supported).   tag_on_invalid array Specified tag to be attached to request context after an invalid request error occurs.    "});index.add({'id':35,'href':'/docs/references/filters/cache/','title':"cache",'section':"Online Filter",'content':"cache #  Description #  The cache filter is composed of the get_cache and set_cache filters, which need to be used in combination. The cache filter is used to cache accelerated queries, prevent repeated requests, and reduce the query pressure of back-end clusters.\nget_cache Filter #  The get_cache filter is used to acquire previous messages from the cache and return them to the client, without needing to access the back-end Elasticsearch. It is intended to cache hotspot data.\nA configuration example is as follows:\nflow: - name: get_cache filter: - get_cache: pass_patterns: [\u0026quot;_cat\u0026quot;,\u0026quot;scroll\u0026quot;, \u0026quot;scroll_id\u0026quot;,\u0026quot;_refresh\u0026quot;,\u0026quot;_cluster\u0026quot;,\u0026quot;_ccr\u0026quot;,\u0026quot;_count\u0026quot;,\u0026quot;_flush\u0026quot;,\u0026quot;_ilm\u0026quot;,\u0026quot;_ingest\u0026quot;,\u0026quot;_license\u0026quot;,\u0026quot;_migration\u0026quot;,\u0026quot;_ml\u0026quot;,\u0026quot;_rollup\u0026quot;,\u0026quot;_data_stream\u0026quot;,\u0026quot;_open\u0026quot;, \u0026quot;_close\u0026quot;] Parameter Description #     Name Type Description     pass_patterns string Rule for ignoring the cache for a request. The cache is skipped when the URL contains any defined keyword.    set_cache Filter #  The set_cache filter is used to cache results returned through back-end query. Expiration time can be set for the cache.\nA configuration example is as follows:\nflow: - name: get_cache filter: - set_cache: min_response_size: 100 max_response_size: 1024000 cache_ttl: 30s max_cache_items: 100000 Parameter Description #     Name Type Description     cache_type string Cache type. It can be set to ristretto, ccache, or redis, and the default value is ristretto.   cache_ttl string Expiration time of the cache. The default value is 10s.   async_search_cache_ttl string Expiration time of the cache for storing asynchronous request results. The default value is 10m.   min_response_size int Minimum message body size that meets cache requirements. The default value is -1, indicating an unlimited value.   max_response_size int Maximum message body size that meets cache requirements. The default value is the maximum value of the int parameter.   max_cached_item int Maximum number of messages that can be cached. The default value is 1000000. The value is valid when the cache type is ccache.   max_cached_size int Maximum cache memory overhead. The default value is 1000000000, that is, 1 GB. The value is valid when the cache type is ristretto.   validated_status_code array Request status code that is allowed to be cached. The default value is 200,201,404,403,413,400,301.    Other Parameters #  If you want to ignore caching, you can define no_cache in the URL parameters to cause the gateway to ignore caching. For example:\ncurl http://localhost:8000/_search?no_cache=true "});index.add({'id':36,'href':'/docs/references/filters/clone/','title':"clone",'section':"Online Filter",'content':"clone #  Description #  The clone filter is used to clone and forward traffic to another handling flow. It can implement dual-write, multi-write, multi-DC synchronization, cluster upgrade, version switching, and other requirements.\nConfiguration Example #  A simple example is as follows:\nflow: - name: double_write filter: - clone: flows: - write_to_region_a - write_to_region_b #last one's response will be output to client - name: write_to_region_a filter: - elasticsearch: elasticsearch: es1 - name: write_to_region_b filter: - elasticsearch: elasticsearch: es2 The above example copies Elasticsearch requests to two different remote clusters.\nParameter Description #     Name Type Description     flows array Multiple traffic handling flows, which are executed one after another. The result of the last flow is output to the client.   continue bool Whether to continue the previous flow after traffic is migrated. The gateway returns immediately after it is set to false. The default value is false.    "});index.add({'id':37,'href':'/docs/references/filters/context_filter/','title':"context_filter",'section':"Online Filter",'content':"context_filter #  Description #  The context_filter is used to filter traffic by request context.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - context_filter: context: _ctx.request.path message: \u0026quot;request not allowed.\u0026quot; status: 403 must: #must match all rules to continue prefix: - /medcl contain: - _search suffix: - _count - _refresh wildcard: - /*/_refresh regex: - ^/m[\\w]+dcl must_not: # any match will be filtered prefix: - /.kibana - /_security - /_security - /gateway_requests* - /.reporting - /_monitoring/bulk contain: - _search suffix: - _count - _refresh wildcard: - /*/_refresh regex: - ^/m[\\w]+dcl should: prefix: - /medcl contain: - _search - _async_search suffix: - _refresh wildcard: - /*/_refresh regex: - ^/m[\\w]+dcl Parameter Description #     Name Type Description     context string Context variable   exclude array List of variables used to refuse requests to pass through   include array List of variables used to allow requests to pass through   must.* object Requests are allowed to pass through only when all conditions are met.   must_not.* object Requests are allowed to pass through only when none of the conditions are met.   should.* object Requests are allowed to pass through when any condition is met.   *.prefix array Whether a request begins with a specific character   *.suffix array Whether a request ends with a specific character   *.contain array Whether a request contains a specific character   *.wildcard array Whether a request meets pattern matching rules   *.regex array Whether a request meets regular expression matching rules   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If only the should condition is met, requests are allowed to pass through only when at least one item in should is met.\n"});index.add({'id':38,'href':'/docs/references/filters/context_limiter/','title':"context_limiter",'section':"Online Filter",'content':"context_limiter #  Description #  The context_limiter filter is used to control the traffic based on request context.\nConfiguration Example #  A configuration example is as follows:\nflow: - name: default_flow filter: - context_limiter: max_requests: 1 action: drop context: - _ctx.request.path - _ctx.request.header.Host - _ctx.request.header.Env The above configuration combines three context variables (_ctx.request.path, _ctx.request.header.Host, and _ctx.request.header.Env) into a bucket for traffic control. The allowable maximum queries per second (QPS) is 1 per second. Subsequent requests out of the traffic control range are directly denied.\nParameter Description #     Name Type Description     context array Context variables, which form a bucket key   interval string Interval for evaluating whether traffic control conditions are met. The default value is 1s.   max_requests int Maximum request count limit in the interval   max_bytes int Maximum request traffic limit in the interval   action string Processing action after traffic control is triggered. The value can be set as retry or drop and the default value is retry.   status string Status code returned after traffic control conditions are met. The default value is 429.   message string Rejection message returned for a request, for which traffic control conditions are met   retry_interval int Interval for traffic control retry, in milliseconds. The default value is 10.   max_retry_times int Maximum retry count in the case of traffic control retries. The default value is 1000.   failed_retry_message string Rejection message returned for a request, for which the maximum retry count has been reached    "});index.add({'id':39,'href':'/docs/references/filters/context_regex_replace/','title':"context_regex_replace",'section':"Online Filter",'content':"context_regex_replace #  Description #  The context_regex_replace filter is used to replace and modify relevant information in the request context by using regular expressions.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - context_regex_replace: context: \u0026quot;_ctx.request.path\u0026quot; pattern: \u0026quot;^/\u0026quot; to: \u0026quot;/cluster:\u0026quot; when: contains: _ctx.request.path: /_search - dump: request: true This example replaces curl localhost:8000/abc/_search in requests with curl localhost:8000/cluster:abc/_search.\nParameter Description #     Name Type Description     context string Request context and corresponding key   pattern string Regular expression used for matching and replacement   to string Target string used for replacement    A list of context variables that can be modified is provided below:\n   Name Type Description     _ctx.request.uri string Complete URL of a request   _ctx.request.path string Request path   _ctx.request.host string Request host   _ctx.request.body string Request body   _ctx.request.body_json.[JSON_PATH] string Path to the JSON request object   _ctx.request.query_args.[KEY] string URL query request parameter   _ctx.request.header.[KEY] string Request header information   _ctx.response.header.[KEY] string Response header information   _ctx.response.body string Returned response body   _ctx.response.body_json.[JSON_PATH] string Path to the JSON response object    "});index.add({'id':40,'href':'/docs/references/processors/dag/','title':"dag",'section':"Offline Processor",'content':"dag #  Description #  The dag processor is used to manage the concurrent scheduling of tasks.\nConfiguration Example #  The following example defines a service named racing_example and auto_start is set to true. Processing units to be executed in sequence are set in processor, the dag processor supports concurrent execution of multiple tasks and the wait_all and first_win aggregation modes.\npipeline: - name: racing_example auto_start: true processor: - echo: #ready, set, go message: read,set,go - dag: mode: wait_all #first_win, wait_all parallel: - echo: #player1 message: player1 - echo: #player2 message: player2 - echo: #player3 message: player3 end: - echo: #checking score message: checking score - echo: #announce champion message: 'announce champion' - echo: #done message: racing finished The echo processor above is very simple and is used to output a specified message. This pipeline simulates a race scene, in which players 1, 2, and 3 run at the same time. After they run, the scores are calculated and the winner is announced, and finally the completion information is output. The output of the program is as follows:\n[10-12 14:59:22] [INF] [echo.go:36] message:read,set,go [10-12 14:59:22] [INF] [echo.go:36] message:player1 [10-12 14:59:22] [INF] [echo.go:36] message:player2 [10-12 14:59:22] [INF] [echo.go:36] message:player3 [10-12 14:59:22] [INF] [echo.go:36] message:checking score [10-12 14:59:22] [INF] [echo.go:36] message:announce champion [10-12 14:59:22] [INF] [echo.go:36] message:racing finished Parameter Description #     Name Type Description     mode string Aggregation mode of task results. The value first_win indicates that the program continues further execution after any of the concurrent tasks is completed, and the value wait_all indicates that the program continues further execution only after all concurrent tasks are completed.   parallel array Task array list, in which multiple subtasks are defined in sequence.   end array Task array list, which lists tasks to be executed after concurrent tasks are completed.    "});index.add({'id':41,'href':'/docs/references/filters/date_range_precision_tuning/','title':"date_range_precision_tuning",'section':"Online Filter",'content':"date_range_precision_tuning #  Description #  The date_range_precision_tuning filter is used to reset the time precision for time range query. After the precision is adjusted, adjacent repeated requests initiated within a short period of time can be easily cached. For scenarios with low time precision but a large amount of data, for example, if Kibana is used for report analysis, you can reduce the precision to cache repeated query requests to reduce the pressure of the back-end server and accelerate the front-end report presentation.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - date_range_precision_tuning: time_precision: 4 - get_cache: - elasticsearch: elasticsearch: dev - set_cache: Precision Description #  Queries sent by Kibana to Elasticsearch use the current time (Now) by default, which is accurate to milliseconds. You can set different precision levels to rewrite queries. See the following query example:\n{\u0026quot;range\u0026quot;:{\u0026quot;@timestamp\u0026quot;:{\u0026quot;gte\u0026quot;:\u0026quot;2019-09-26T08:21:12.152Z\u0026quot;,\u0026quot;lte\u0026quot;:\u0026quot;2020-09-26T08:21:12.152Z\u0026quot;,\u0026quot;format\u0026quot;:\u0026quot;strict_date_optional_time\u0026quot;} Set different precision levels. The query results after rewriting are as follows:\n   Precision New Query     0 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T00:00:00.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T23:59:59.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   1 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T00:00:00.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T09:59:59.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   2 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:00:00.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:59:59.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   3 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:20:00.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:29:59.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   4 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:21:00.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:21:59.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   5 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:21:10.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:21:19.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   6 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:21:12.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:21:12.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   7 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:21:12.100Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:21:12.199Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   8 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:21:12.150Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:21:12.159Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   9 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:21:12.152Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:21:12.152Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}    Parameter Description #     Name Type Description     time_precision int Precision length of time, that is, the digit length of displayed time. The default value is 4 and the valid range is from 0 to 9.   path_keywords array Keyword contained in a request. The time precision is reset only for requests that contain the keywords, to prevent parsing of unnecessary requests. The default values are _search and _async_search.    "});index.add({'id':42,'href':'/docs/references/filters/drop/','title':"drop",'section':"Online Filter",'content':"drop #  Description #  The drop filter is used to discard a message and end the processing of a request in advance.\nConfiguration Example #  A simple example is as follows:\nflow: - name: drop filter: - drop: "});index.add({'id':43,'href':'/docs/references/filters/dump/','title':"dump",'section':"Online Filter",'content':"dump #  Description #  The dump filter is used to dump relevant request information on terminals. It is mainly used for debugging.\nConfiguration Example #  A simple example is as follows:\nflow: - name: hello_world filter: - dump: uri: true request_header: true request_body: true response_body: true status_code: true Parameter Description #  The dump filter is relatively simple. After the dump filter is inserted into a required flow handling phase, the terminal can output request information about the phase, facilitating debugging.\n   Name Type Description     request bool Whether to output all complete request information   uri bool Whether to output the requested URI information   query_args bool Whether to output the requested parameter information   user bool Whether to output the requested user information   api_key bool Whether to output the requested API key information   request_header bool Whether to output the header information of the request   response_header bool Whether to output the header information of the response   status_code bool Whether to output the status code of the response   context array User-defined context information for output    Outputting Context #  You can use the context parameter to debug request context information. The following is an example of the configuration file.\nflow: - name: echo filter: - set_response: status: 201 content_type: \u0026quot;text/plain; charset=utf-8\u0026quot; body: \u0026quot;hello world\u0026quot; - set_response_header: headers: - Env -\u0026gt; Dev - dump: context: - _ctx.id - _ctx.tls - _ctx.remote_addr - _ctx.local_addr - _ctx.request.host - _ctx.request.method - _ctx.request.uri - _ctx.request.path - _ctx.request.body - _ctx.request.body_length - _ctx.request.query_args.from - _ctx.request.query_args.size - _ctx.request.header.Accept - _ctx.request.user - _ctx.response.status - _ctx.response.body - _ctx.response.content_type - _ctx.response.body_length - _ctx.response.header.Env Start the gateway and run the following command:\ncurl http://localhost:8000/medcl/_search\\?from\\=1\\\u0026amp;size\\=100 -d'{search:query123}' -v -u 'medcl:123' The gateway outputs the following information:\n---- dumping context ---- _ctx.id : 21474836481 _ctx.tls : false _ctx.remote_addr : 127.0.0.1:50925 _ctx.local_addr : 127.0.0.1:8000 _ctx.request.host : localhost:8000 _ctx.request.method : POST _ctx.request.uri : http://localhost:8000/medcl/_search?from=1\u0026amp;size=100 _ctx.request.path : /medcl/_search _ctx.request.body : {search:query123} _ctx.request.body_length : 17 _ctx.request.query_args.from : 1 _ctx.request.query_args.size : 100 _ctx.request.header.Accept : */* _ctx.request.user : medcl _ctx.response.status : 201 _ctx.response.body : hello world _ctx.response.content_type : text/plain; charset=utf-8 _ctx.response.body_length : 11 _ctx.response.header.Env : Dev "});index.add({'id':44,'href':'/docs/references/processors/dump_hash/','title':"dump_hash",'section':"Offline Processor",'content':"dump_hash #  Description #  The dump_hash processor is used to export index documents of a cluster and calculate the hash value.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - dump_hash: #dump es1's doc indices: \u0026quot;medcl-dr3\u0026quot; scroll_time: \u0026quot;10m\u0026quot; elasticsearch: \u0026quot;source\u0026quot; query: \u0026quot;field1:elastic\u0026quot; fields: \u0026quot;doc_hash\u0026quot; output_queue: \u0026quot;source_docs\u0026quot; batch_size: 10000 slice_size: 5 Parameter Description #     Name Type Description     elasticsearch string Name of a target cluster   scroll_time string Scroll session timeout duration   batch_size int Scroll batch size, which is set to 5000 by default   slice_size int Slice size, which is set to 1 by default   sort_type string Document sorting type, which is set to asc by default   sort_field string Document sorting field   indices string Index   level string Request processing level, which can be set to cluster, indicating that node- and shard-level splitting are not performed on requests. It is applicable to scenarios in which there is a proxy in front of Elasticsearch.   query string Query filter conditions   fields string List of fields to be returned   sort_document_fields bool Whether to sort fields in _source before the hash value is calculated. The default value is false.   hash_func string Hash function, which can be set to xxhash32, xxhash64, or fnv1a. The default value is xxhash32.   output_queue string Name of a queue that outputs results    "});index.add({'id':45,'href':'/docs/references/filters/elasticsearch/','title':"elasticsearch",'section':"Online Filter",'content':"elasticsearch #  Description #  The elasticsearch filter is used to forward requests to back-end Elasticsearch clusters.\nConfiguration Example #  Before using the elasticsearch filter, define one Elasticsearch cluster configuration node as follows:\nelasticsearch: - name: prod enabled: true endpoint: http://192.168.3.201:9200 The following shows a flow configuration example.\nflow: - name: cache_first filter: - elasticsearch: elasticsearch: prod The preceding example forwards requests to the prod cluster.\nAutomatic Update #  For a large cluster that contains many nodes, it is almost impossible to configure all back-end nodes individually. Instead, you only need to enable auto-discovery of back-end nodes on the Elasticsearch module. See the following example.\nelasticsearch: - name: prod enabled: true endpoint: http://192.168.3.201:9200 discovery: enabled: true refresh: enabled: true basic_auth: username: elastic password: pass Then, enable automatic configuration refresh on the filter. Now, all back-end nodes can be accessed and the status of online and offline nodes is automatically updated. See the following example.\nflow: - name: cache_first filter: - elasticsearch: elasticsearch: prod refresh: enabled: true interval: 30s Setting the Weight #  If there are many back-end clusters, INFINI Gateway allows you to set different access weights for different nodes. See the following configuration example.\nflow: - name: cache_first filter: - elasticsearch: elasticsearch: prod balancer: weight refresh: enabled: true interval: 30s weights: - host: 192.168.3.201:9200 weight: 10 - host: 192.168.3.202:9200 weight: 20 - host: 192.168.3.203:9200 weight: 30 In the above example, the traffic destined for an Elasticsearch cluster is distributed to the 203, 202, and 201 nodes at a ratio of 3：2：1.\nFiltering Node #  INFINI Gateway can also filter requests based on node IP address, label, or role to avoid sending requests to specific nodes, such as the master and cold nodes. See the following configuration example.\nflow: - name: cache_first filter: - elasticsearch: elasticsearch: prod balancer: weight refresh: enabled: true interval: 30s filter: hosts: exclude: - 192.168.3.201:9200 include: - 192.168.3.202:9200 - 192.168.3.203:9200 tags: exclude: - temp: cold include: - disk: ssd roles: exclude: - master include: - data - ingest Parameter Description #     Name Type Description     elasticsearch string Name of an Elasticsearch cluster   max_connection_per_node int Maximum number of TCP connections that are allowed to access each node of an Elasticsearch cluster. The default value is 5000.   max_response_size int Maximum size of the message body returned in response to an Elasticsearch request. The default value is 100*1024*1024.   max_conn_wait_timeout int Timeout duration for Elasticsearch to wait for an idle connection. The default value is 10s.   max_idle_conn_duration int Idle duration of an Elasticsearch connection. The default value is 0s.   max_conn_duration int Duration of an Elasticsearch connection. The default value is 0s.   read_timeout int Read timeout duration of an Elasticsearch request. The default value is 0s.   write_timeout int Write timeout duration of an Elasticsearch request. The default value is 0s.   read_buffer_size int Read cache size for an Elasticsearch request. The default value is 4096*4.   write_buffer_size int Write cache size for an Elasticsearch request. The default value is 4096*4.   tls_insecure_skip_verify bool Whether to ignore TLS certificate verification of an Elasticsearch cluster. The default value is true.   balancer string Load balancing algorithm of a back-end Elasticsearch node. Currently, only the weight weight-based algorithm is available.   refresh.enable bool Whether to enable automatic refresh of node status changes, to perceive changes in the back-end Elasticsearch topology   refresh.interval int Interval of the node status refresh   weights array Priority of a back-end node. A node with a larger weight is assigned a higher proportion of request forwarding.   filter object Filtering rules for back-end Elasticsearch nodes. Rules can be set to forward requests to a specific node.   filter.hosts object Filtering based on the access address of Elasticsearch   filter.tags object Filtering based on the label of Elasticsearch   filter.roles object Filtering based on the role of Elasticsearch   filter.*.exclude array Conditions for excluding. Any matched node is denied handling requests as a proxy.   filter.*.include array Elasticsearch nodes that meet conditions are allowed to handle requests as a proxy. When the exclude parameter is not configured but include is configured, any condition in include must be met. Otherwise, the node is not allowed to handle requests as a proxy.    "});index.add({'id':46,'href':'/docs/references/filters/elasticsearch_health_check/','title':"elasticsearch_health_check",'section':"Online Filter",'content':"elasticsearch_health_check #  Description #  The elasticsearch_health_check filter is used to detect the health status of Elasticsearch in traffic control mode. When a back-end fault occurs, the filter triggers an active cluster health check without waiting for the results of the default polling check of Elasticsearch. Traffic control can be configured to enable the filter to send check requests to the back-end Elasticsearch at a maximum of once per second.\nConfiguration Example #  A simple example is as follows:\nflow: - name: elasticsearch_health_check filter: - elasticsearch_health_check: elasticsearch: dev Parameter Description #     Name Type Description     elasticsearch string Cluster ID   interval int Minimum interval for executing requests, in seconds. The default value is 1.    "});index.add({'id':47,'href':'/docs/references/filters/flow/','title':"flow",'section':"Online Filter",'content':"flow #  Description #  The flow filter is used to redirect to or execute one or a series of other flows.\nConfiguration Example #  A simple example is as follows:\nflow: - name: flow filter: - flow: flows: - request_logging Parameter Description #     Name Type Description     flows array Flow ID, in the array format. You can specify multiple flows, which are executed in sequence.    "});index.add({'id':48,'href':'/docs/references/processors/flow_runner/','title':"flow_runner",'section':"Offline Processor",'content':"flow_runner #  Description #  The flow_runner processor is used to asynchronously consume requests in a queue by using the processing flow used for online requests.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - flow_runner: input_queue: \u0026quot;primary_deadletter_requests\u0026quot; flow: primary-flow-post-processing when: cluster_available: [ \u0026quot;primary\u0026quot; ] Parameter Description #     Name Type Description     input_queue string Name of a subscribed queue   flow string Flow used to consume requests in consumption queues   commit_on_tag string A message is committed only when a specified tag exists in the context of the current request. The default value is blank, indicating that a message is committed immediately after the execution is complete.    "});index.add({'id':49,'href':'/docs/references/filters/http/','title':"http",'section':"Online Filter",'content':"http #  Description #  The http filter is used to forward requests to a specified HTTP server as a proxy.\nConfiguration Example #  A simple example is as follows:\nflow: - name: default_flow filter: - basic_auth: valid_users: medcl: passwd - http: schema: \u0026quot;http\u0026quot; #https or http #host: \u0026quot;192.168.3.98:5601\u0026quot; hosts: - \u0026quot;192.168.3.98:5601\u0026quot; - \u0026quot;192.168.3.98:5602\u0026quot; Parameter Description #     Name Type Description     timeout_in_second int Request timeout duration, in seconds. The default value is 10.   schema string http or https   host string Target host address containing the port ID, for example, localhost:9200   hosts array Host address list. The addresses are tried in sequence after a fault occurs.    "});index.add({'id':50,'href':'/docs/references/processors/index_diff/','title':"index_diff",'section':"Offline Processor",'content':"index_diff #  Description #  The index_diff processor is used to compare differences between two result sets.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - index_diff: diff_queue: \u0026quot;diff_result\u0026quot; buffer_size: 1 text_report: true #If data needs to be saved to Elasticsearch, disable the function and start the diff_result_ingest task of the pipeline. source_queue: 'source_docs' target_queue: 'target_docs' Parameter Description #     Name Type Description     source_queue string Name of source data   target_queue string Name of target data   diff_queue string Queue that stores difference results   buffer_size int Memory buffer size   keep_source bool Whether difference results contain document source information   text_report bool Whether to output results in text form    "});index.add({'id':51,'href':'/docs/references/processors/json_indexing/','title':"json_indexing",'section':"Offline Processor",'content':"json_indexing #  Description #  The json_indexing processor is used to consume pure JSON documents in queues and store them to a specified Elasticsearch server.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: request_logging_index auto_start: true keep_running: true processor: - json_indexing: index_name: \u0026quot;gateway_requests\u0026quot; elasticsearch: \u0026quot;dev\u0026quot; input_queue: \u0026quot;request_logging\u0026quot; idle_timeout_in_seconds: 1 worker_size: 1 bulk_size_in_mb: 10 Parameter Description #     Name Type Description     input_queue int Name of a subscribed queue   worker_size int Number of threads that concurrently execute consumption tasks, which is set to 1 by default.   idle_timeout_in_seconds int Timeout duration of the consumption queue, in seconds. The default value is 5.   bulk_size_in_kb int Size of a bulk request, in KB.   bulk_size_in_mb int Size of a bulk request, in MB.   elasticsearch string Name of a target cluster, to which requests are saved.   index_name string Name of the index stored to the target cluster.   type_name string Name of the index type stored to the target cluster. It is set based on the cluster version. The value is doc for Elasticsearch versions earlier than v7 and _doc for versions later than v7.    "});index.add({'id':52,'href':'/docs/references/filters/ldap_auth/','title':"ldap_auth",'section':"Online Filter",'content':"ldap_auth #  Description #  The ldap_auth filter is used to set authentication based on the Lightweight Directory Access Protocol (LDAP).\nConfiguration Example #  A simple example is as follows:\nflow: - name: ldap_auth filter: - ldap_auth: host: \u0026quot;ldap.forumsys.com\u0026quot; port: 389 bind_dn: \u0026quot;cn=read-only-admin,dc=example,dc=com\u0026quot; bind_password: \u0026quot;password\u0026quot; base_dn: \u0026quot;dc=example,dc=com\u0026quot; user_filter: \u0026quot;(uid=%s)\u0026quot; The above configuration uses an online free LDAP test server, the test user is tesla, and the password is password.\n➜ curl http://127.0.0.1:8000/ -u tesla:password { \u0026quot;name\u0026quot; : \u0026quot;192.168.3.7\u0026quot;, \u0026quot;cluster_name\u0026quot; : \u0026quot;elasticsearch\u0026quot;, \u0026quot;cluster_uuid\u0026quot; : \u0026quot;ZGTwWtBfSLWRpsS1VKQDiQ\u0026quot;, \u0026quot;version\u0026quot; : { \u0026quot;number\u0026quot; : \u0026quot;7.8.0\u0026quot;, \u0026quot;build_flavor\u0026quot; : \u0026quot;default\u0026quot;, \u0026quot;build_type\u0026quot; : \u0026quot;tar\u0026quot;, \u0026quot;build_hash\u0026quot; : \u0026quot;757314695644ea9a1dc2fecd26d1a43856725e65\u0026quot;, \u0026quot;build_date\u0026quot; : \u0026quot;2020-06-14T19:35:50.234439Z\u0026quot;, \u0026quot;build_snapshot\u0026quot; : false, \u0026quot;lucene_version\u0026quot; : \u0026quot;8.5.1\u0026quot;, \u0026quot;minimum_wire_compatibility_version\u0026quot; : \u0026quot;6.8.0\u0026quot;, \u0026quot;minimum_index_compatibility_version\u0026quot; : \u0026quot;6.0.0-beta1\u0026quot; }, \u0026quot;tagline\u0026quot; : \u0026quot;You Know, for Search\u0026quot; } ➜ curl http://127.0.0.1:8000/ -u tesla:password1 Unauthorized% Parameter Description #     Name Type Description     host string Address of the LDAP server   port int Port of the LDAP server. The default value is 389.   tls bool Whether the LDAP server uses the Transport Layer Security (TLS) protocol. The default value is false.   bind_dn string Information about the user who performs the LDAP query   bind_password string Password for performing the LDAP query   base_dn string Root domain for filtering LDAP users   user_filter string Query condition for filtering LDAP users. The default value is (uid=%s).   uid_attribute string Attribute of a user ID. The default value is uid.   group_attribute string Attribute of a user group. The default value is cn.   attribute array List of attributes returned by the LDAP query    "});index.add({'id':53,'href':'/docs/references/filters/logging/','title':"logging",'section':"Online Filter",'content':"logging #  Description #  The logging filter is used to asynchronously record requests to the local disk to minimize the delay of requests. In scenarios with heavy traffic, you are advised to use other request filters jointly to reduce the total number of logs.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - logging: queue_name: request_logging An example of a recorded request log is as follows:\n { \u0026quot;_index\u0026quot; : \u0026quot;gateway_requests\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;EH5bG3gBsbC2s3iWFzCF\u0026quot;, \u0026quot;_score\u0026quot; : 1.0, \u0026quot;_source\u0026quot; : { \u0026quot;tls\u0026quot; : false, \u0026quot;@timestamp\u0026quot; : \u0026quot;2021-03-10T08:57:30.645Z\u0026quot;, \u0026quot;conn_time\u0026quot; : \u0026quot;2021-03-10T08:57:30.635Z\u0026quot;, \u0026quot;flow\u0026quot; : { \u0026quot;from\u0026quot; : \u0026quot;127.0.0.1\u0026quot;, \u0026quot;process\u0026quot; : [ \u0026quot;request_body_regex_replace\u0026quot;, \u0026quot;get_cache\u0026quot;, \u0026quot;date_range_precision_tuning\u0026quot;, \u0026quot;get_cache\u0026quot;, \u0026quot;elasticsearch\u0026quot;, \u0026quot;set_cache\u0026quot;, \u0026quot;||\u0026quot;, \u0026quot;request_logging\u0026quot; ], \u0026quot;relay\u0026quot; : \u0026quot;192.168.43.101-Quartz\u0026quot;, \u0026quot;to\u0026quot; : [ \u0026quot;localhost:9200\u0026quot; ] }, \u0026quot;id\u0026quot; : 3, \u0026quot;local_ip\u0026quot; : \u0026quot;127.0.0.1\u0026quot;, \u0026quot;remote_ip\u0026quot; : \u0026quot;127.0.0.1\u0026quot;, \u0026quot;request\u0026quot; : { \u0026quot;body_length\u0026quot; : 53, \u0026quot;body\u0026quot; : \u0026quot;\u0026quot;\u0026quot; { \u0026quot;query\u0026quot;: { \u0026quot;match_all\u0026quot;: {} },\u0026quot;size\u0026quot;: 100 } \u0026quot;\u0026quot;\u0026quot;, \u0026quot;header\u0026quot; : { \u0026quot;content-type\u0026quot; : \u0026quot;application/json\u0026quot;, \u0026quot;User-Agent\u0026quot; : \u0026quot;curl/7.54.0\u0026quot;, \u0026quot;Accept\u0026quot; : \u0026quot;*/*\u0026quot;, \u0026quot;Host\u0026quot; : \u0026quot;localhost:8000\u0026quot;, \u0026quot;content-length\u0026quot; : \u0026quot;53\u0026quot; }, \u0026quot;host\u0026quot; : \u0026quot;localhost:8000\u0026quot;, \u0026quot;local_addr\u0026quot; : \u0026quot;127.0.0.1:8000\u0026quot;, \u0026quot;method\u0026quot; : \u0026quot;POST\u0026quot;, \u0026quot;path\u0026quot; : \u0026quot;/myindex/_search\u0026quot;, \u0026quot;remote_addr\u0026quot; : \u0026quot;127.0.0.1:63309\u0026quot;, \u0026quot;started\u0026quot; : \u0026quot;2021-03-10T08:57:30.635Z\u0026quot;, \u0026quot;uri\u0026quot; : \u0026quot;http://localhost:8000/myindex/_search\u0026quot; }, \u0026quot;response\u0026quot; : { \u0026quot;body_length\u0026quot; : 441, \u0026quot;cached\u0026quot; : false, \u0026quot;elapsed\u0026quot; : 9.878, \u0026quot;status_code\u0026quot; : 200, \u0026quot;body\u0026quot; : \u0026quot;\u0026quot;\u0026quot;{\u0026quot;took\u0026quot;:0,\u0026quot;timed_out\u0026quot;:false,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;successful\u0026quot;:1,\u0026quot;skipped\u0026quot;:0,\u0026quot;failed\u0026quot;:0},\u0026quot;hits\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;max_score\u0026quot;:1.0,\u0026quot;hits\u0026quot;:[{\u0026quot;_index\u0026quot;:\u0026quot;myindex\u0026quot;,\u0026quot;_type\u0026quot;:\u0026quot;doc\u0026quot;,\u0026quot;_id\u0026quot;:\u0026quot;c132mhq3r0otidqkac1g\u0026quot;,\u0026quot;_score\u0026quot;:1.0,\u0026quot;_source\u0026quot;:{\u0026quot;name\u0026quot;:\u0026quot;local\u0026quot;,\u0026quot;enabled\u0026quot;:true,\u0026quot;endpoint\u0026quot;:\u0026quot;http://localhost:9200\u0026quot;,\u0026quot;basic_auth\u0026quot;:{},\u0026quot;discovery\u0026quot;:{\u0026quot;refresh\u0026quot;:{}},\u0026quot;created\u0026quot;:\u0026quot;2021-03-08T21:48:55.687557+08:00\u0026quot;,\u0026quot;updated\u0026quot;:\u0026quot;2021-03-08T21:48:55.687557+08:00\u0026quot;}}]}}\u0026quot;\u0026quot;\u0026quot;, \u0026quot;header\u0026quot; : { \u0026quot;UPSTREAM\u0026quot; : \u0026quot;localhost:9200\u0026quot;, \u0026quot;process\u0026quot; : \u0026quot;request_body_regex_replace-\u0026gt;get_cache-\u0026gt;date_range_precision_tuning-\u0026gt;get_cache-\u0026gt;elasticsearch-\u0026gt;set_cache\u0026quot;, \u0026quot;content-length\u0026quot; : \u0026quot;441\u0026quot;, \u0026quot;content-type\u0026quot; : \u0026quot;application/json; charset=UTF-8\u0026quot;, \u0026quot;Server\u0026quot; : \u0026quot;INFINI\u0026quot;, \u0026quot;CLUSTER\u0026quot; : \u0026quot;dev\u0026quot; }, \u0026quot;local_addr\u0026quot; : \u0026quot;127.0.0.1:63310\u0026quot; } } } Parameter Description #     Name Type Description     queue_name string Name of a queue that stores request logs in the local disk   format_header_keys bool Whether to standardize the header and convert it into lowercase letters. The default value is false.   remove_authorization bool Whether to remove authorization information from the header. The default value is true.   max_request_body_size int Whether to truncate a very long request message. The default value is 1024, indicating that 1024 characters are retained.   max_response_body_size int Whether to truncate a very long response message. The default value is 1024, indicating that 1024 characters are retained.   min_elapsed_time_in_ms int Request filtering based on response time, that is, the minimum time (ms) for request logging. A request with time that exceeds this value will be logged.   bulk_stats_details bool Whether to record detailed index-based bulk request statistics. The default value is true.    "});index.add({'id':54,'href':'/docs/references/filters/queue/','title':"queue",'section':"Online Filter",'content':"queue #  Description #  The queue filter is used to store requests to message queues.\nConfiguration Example #  A simple example is as follows:\nflow: - name: queue filter: - queue: queue_name: queue_name Parameter Description #     Name Type Description     depth_threshold int Queue depth threshold. Only a request with the size exceeding this value can be stored to a queue. The default value is 0.   queue_name string Name of a message queue    "});index.add({'id':55,'href':'/docs/references/processors/queue_consumer/','title':"queue_consumer",'section':"Offline Processor",'content':"queue_consumer #  Description #  The queue_consumer processor is used to asynchronously consume requests in a queue and send the requests to Elasticsearch.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - queue_consumer: input_queue: \u0026quot;backup\u0026quot; elasticsearch: \u0026quot;backup\u0026quot; waiting_after: [ \u0026quot;backup_failure_requests\u0026quot;] worker_size: 20 when: cluster_available: [ \u0026quot;backup\u0026quot; ] Parameter Description #     Name Type Description     input_queue int Name of a subscribed queue   worker_size int Number of threads that concurrently execute consumption tasks, which is set to 1 by default.   idle_timeout_in_seconds int Timeout duration of the consumption queue, which is set to 1 by default.   elasticsearch string Name of a target cluster, to which requests are saved.   waiting_after array Data in the main queue can be consumed only after data in a specified queue is consumed.   failure_queue string Request that fails to be executed because of a back-end failure. The default value is %input_queue%-failure.   invalid_queue string Request, for which the returned status code is 4xx. The default value is %input_queue%-invalid.   compress bool Whether to compress requests. The default value is false.   safety_parse bool Whether to enable secure parsing, that is, no buffer is used and memory usage is higher. The default value is true.   doc_buffer_size bool Maximum document buffer size for the processing of a single request. You are advised to set it to be greater than the maximum size of a single document. The default value is 256*1024.    "});index.add({'id':56,'href':'/docs/references/filters/ratio/','title':"ratio",'section':"Online Filter",'content':"ratio #  Description #  The ratio filter is used to forward normal traffic to another flow proportionally. It can implement canary release, traffic migration and export, or switch some traffic to clusters of different versions for testing.\nConfiguration Example #  A simple example is as follows:\nflow: - name: ratio_traffic_forward filter: - ratio: ratio: 0.1 flow: hello_world continue: true Parameter Description #     Name Type Description     ratio float Proportion of traffic to be migrated   flow string New traffic processing flow   continue bool Whether to continue the previous flow after traffic is migrated. The gateway returns immediately after it is set to false. The default value is false.    "});index.add({'id':57,'href':'/docs/references/filters/record/','title':"record",'section':"Online Filter",'content':"record #  Description #  The record filter is used to record requests. Output requests can be copied to the console of Kibana for debugging.\nConfiguration Example #  A simple example is as follows:\nflow: - name: request_logging filter: - record: stdout: true filename: requests.txt Examples of the format of request logs output by the record filter are as follows:\nGET /_cluster/state/version,master_node,routing_table,metadata/* GET /_alias GET /_cluster/health GET /_cluster/stats GET /_nodes/0NSvaoOGRs2VIeLv3lLpmA/stats Parameter Description #     Name Type Description     filename string Filename of request logs stored in the data directory   stdout bool Whether the terminal also outputs the characters. The default value is false.    "});index.add({'id':58,'href':'/docs/references/filters/redis_pubsub/','title':"redis_pubsub",'section':"Online Filter",'content':"redis_pubsub #  Description #  The redis filter is used to store received requests and response results to Redis message queues.\nConfiguration Example #  A simple example is as follows:\nflow: - name: redis_pubsub filter: - redis_pubsub: host: 127.0.0.1 port: 6379 channel: gateway response: true Parameter Description #     Name Type Description     host string Redis host name, which is localhost by default.   port int Redis port ID, which is 6379 by default.   password string Redis password   db int Default database of Redis, which is 0 by default.   channel string Name of a Redis message queue. It is mandatory and has no default value.   response bool Whether the response result is contained. The default value is true.    "});index.add({'id':59,'href':'/docs/references/processors/replay/','title':"replay",'section':"Offline Processor",'content':"replay #  Description #  The replay processor is used to replay requests recorded by the record filter.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: play_requests auto_start: true keep_running: false processor: - replay: filename: requests.txt schema: \u0026quot;http\u0026quot; host: \u0026quot;localhost:8000\u0026quot; Parameter Description #     Name Type Description     filename string Name of a file that contains replayed messages   schema string Request protocol type: http or https   host string Target server that receives requests, in the format of host:port    "});index.add({'id':60,'href':'/docs/references/filters/request_api_key_filter/','title':"request_api_key_filter",'section':"Online Filter",'content':"request_api_key_filter #  Description #  When Elasticsearch conducts authentication through API keys, the request_api_key_filter is used to filter requests based on request API ID.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_api_key_filter: message: \u0026quot;Request filtered!\u0026quot; exclude: - VuaCfGcBCdbkQm-e5aOx The above example shows that requests from VuaCfGcBCdbkQm-e5aOx will be rejected. See the following information.\n➜ ~ curl localhost:8000 -H \u0026quot;Authorization: ApiKey VnVhQ2ZHY0JDZGJrUW0tZTVhT3g6dWkybHAyYXhUTm1zeWFrdzl0dk5udw==\u0026quot; -v * Rebuilt URL to: localhost:8000/ * Trying 127.0.0.1... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 8000 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; Host: localhost:8000 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; Authorization: ApiKey VnVhQ2ZHY0JDZGJrUW0tZTVhT3g6dWkybHAyYXhUTm1zeWFrdzl0dk5udw== \u0026gt; \u0026lt; HTTP/1.1 403 Forbidden \u0026lt; Server: INFINI \u0026lt; Date: Mon, 12 Apr 2021 15:02:37 GMT \u0026lt; content-type: text/plain; charset=utf-8 \u0026lt; content-length: 17 \u0026lt; FILTERED: true \u0026lt; process: request_api_key_filter \u0026lt; * Connection #0 to host localhost left intact {\u0026quot;error\u0026quot;:true,\u0026quot;message\u0026quot;:\u0026quot;Request filtered!\u0026quot;}% ➜ ~ Parameter Description #     Name Type Description     exclude array List of usernames, from which requests are refused to pass through   include array List of usernames, from which requests are allowed to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  "});index.add({'id':61,'href':'/docs/references/filters/request_api_key_limiter/','title':"request_api_key_limiter",'section':"Online Filter",'content':"request_api_key_limiter #  Description #  The request_api_key_limiter filter is used to control traffic by API key.\nConfiguration Example #  A configuration example is as follows:\nflow: - name: rate_limit_flow filter: - request_api_key_limiter: id: - VuaCfGcBCdbkQm-e5aOx max_requests: 1 action: drop # retry or drop message: \u0026quot;your api_key reached our limit\u0026quot; The above configuration controls the traffic with the API ID of VuaCfGcBCdbkQm-e5aOx and the allowable maximum QPS is 1 per second.\n➜ ~ curl localhost:8000 -H \u0026quot;Authorization: ApiKey VnVhQ2ZHY0JDZGJrUW0tZTVhT3g6dWkybHAyYXhUTm1zeWFrdzl0dk5udw==\u0026quot; -v * Rebuilt URL to: localhost:8000/ * Trying 127.0.0.1... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 8000 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; Host: localhost:8000 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; Authorization: ApiKey VnVhQ2ZHY0JDZGJrUW0tZTVhT3g6dWkybHAyYXhUTm1zeWFrdzl0dk5udw== \u0026gt; \u0026lt; HTTP/1.1 429 Too Many Requests \u0026lt; Server: INFINI \u0026lt; Date: Mon, 12 Apr 2021 15:14:52 GMT \u0026lt; content-type: text/plain; charset=utf-8 \u0026lt; content-length: 30 \u0026lt; process: request_api_key_limiter \u0026lt; * Connection #0 to host localhost left intact your api_key reached our limit% Parameter Description #     Name Type Description     id array IDs of APIs that will participate in traffic control. If this parameter is not set, all API keys will participate in traffic control.   interval string Interval for evaluating whether traffic control conditions are met. The default value is 1s.   max_requests int Maximum request count limit in the interval   max_bytes int Maximum request traffic limit in the interval   action string Processing action after traffic control is triggered. The value can be set as retry or drop and the default value is retry.   status string Status code returned after traffic control conditions are met. The default value is 429.   message string Rejection message returned for a request, for which traffic control conditions are met   retry_interval int Interval for traffic control retry, in milliseconds. The default value is 10.   max_retry_times int Maximum retry count in the case of traffic control retries. The default value is 1000.   failed_retry_message string Rejection message returned for a request, for which the maximum retry count has been reached    "});index.add({'id':62,'href':'/docs/references/filters/request_body_json_del/','title':"request_body_json_del",'section':"Online Filter",'content':"request_body_json_del #  Description #  The request_body_json_del filter is used to delete some fields from a request body of the JSON format.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_body_json_del: path: - query.bool.should.[0] - query.bool.must Parameter Description #     Name Type Description     path array JSON path key value to be deleted   ignore_missing bool Whether to ignore processing if the JSON path does not exist. The default value is false.    "});index.add({'id':63,'href':'/docs/references/filters/request_body_json_set/','title':"request_body_json_set",'section':"Online Filter",'content':"request_body_json_set #  Description #  The request_body_json_set filter is used to modify a request body of the JSON format.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_body_json_set: path: - aggs.total_num.terms.field -\u0026gt; \u0026quot;name\u0026quot; - aggs.total_num.terms.size -\u0026gt; 3 - size -\u0026gt; 0 Parameter Description #     Name Type Description     path map It uses -\u0026gt; to identify the key value pair: JSON path and the value used for replacement.   ignore_missing bool Whether to ignore processing if the JSON path does not exist. The default value is false.    "});index.add({'id':64,'href':'/docs/references/filters/request_body_regex_replace/','title':"request_body_regex_replace",'section':"Online Filter",'content':"request_body_regex_replace #  Description #  The request_body_regex_replace filter is used to replace string content in a request body by using a regular expression.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_body_regex_replace: pattern: '\u0026quot;size\u0026quot;: 10000' to: '\u0026quot;size\u0026quot;: 100' - elasticsearch: elasticsearch: prod - dump: request_body: true The above example changes the size from 10000 to 100 in the request body sent to Elasticsearch. The filter can be used to dynamically fix errors or incorrect queries.\nThe test is as follows:\ncurl -XPOST \u0026quot;http://localhost:8000/myindex/_search\u0026quot; -H 'Content-Type: application/json' -d' { \u0026quot;query\u0026quot;: { \u0026quot;match_all\u0026quot;: {} },\u0026quot;size\u0026quot;: 10000 }' The actual query is as follows:\n { \u0026quot;_index\u0026quot; : \u0026quot;gateway_requests\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;EH5bG3gBsbC2s3iWFzCF\u0026quot;, \u0026quot;_score\u0026quot; : 1.0, \u0026quot;_source\u0026quot; : { \u0026quot;tls\u0026quot; : false, \u0026quot;@timestamp\u0026quot; : \u0026quot;2021-03-10T08:57:30.645Z\u0026quot;, \u0026quot;conn_time\u0026quot; : \u0026quot;2021-03-10T08:57:30.635Z\u0026quot;, \u0026quot;flow\u0026quot; : { \u0026quot;from\u0026quot; : \u0026quot;127.0.0.1\u0026quot;, \u0026quot;process\u0026quot; : [ \u0026quot;request_body_regex_replace\u0026quot;, \u0026quot;get_cache\u0026quot;, \u0026quot;date_range_precision_tuning\u0026quot;, \u0026quot;get_cache\u0026quot;, \u0026quot;elasticsearch\u0026quot;, \u0026quot;set_cache\u0026quot;, \u0026quot;||\u0026quot;, \u0026quot;request_logging\u0026quot; ], \u0026quot;relay\u0026quot; : \u0026quot;192.168.43.101-Quartz\u0026quot;, \u0026quot;to\u0026quot; : [ \u0026quot;localhost:9200\u0026quot; ] }, \u0026quot;id\u0026quot; : 3, \u0026quot;local_ip\u0026quot; : \u0026quot;127.0.0.1\u0026quot;, \u0026quot;remote_ip\u0026quot; : \u0026quot;127.0.0.1\u0026quot;, \u0026quot;request\u0026quot; : { \u0026quot;body_length\u0026quot; : 53, \u0026quot;body\u0026quot; : \u0026quot;\u0026quot;\u0026quot; { \u0026quot;query\u0026quot;: { \u0026quot;match_all\u0026quot;: {} },\u0026quot;size\u0026quot;: 100 } \u0026quot;\u0026quot;\u0026quot;, \u0026quot;header\u0026quot; : { \u0026quot;content-type\u0026quot; : \u0026quot;application/json\u0026quot;, \u0026quot;User-Agent\u0026quot; : \u0026quot;curl/7.54.0\u0026quot;, \u0026quot;Accept\u0026quot; : \u0026quot;*/*\u0026quot;, \u0026quot;Host\u0026quot; : \u0026quot;localhost:8000\u0026quot;, \u0026quot;content-length\u0026quot; : \u0026quot;53\u0026quot; }, \u0026quot;host\u0026quot; : \u0026quot;localhost:8000\u0026quot;, \u0026quot;local_addr\u0026quot; : \u0026quot;127.0.0.1:8000\u0026quot;, \u0026quot;method\u0026quot; : \u0026quot;POST\u0026quot;, \u0026quot;path\u0026quot; : \u0026quot;/myindex/_search\u0026quot;, \u0026quot;remote_addr\u0026quot; : \u0026quot;127.0.0.1:63309\u0026quot;, \u0026quot;started\u0026quot; : \u0026quot;2021-03-10T08:57:30.635Z\u0026quot;, \u0026quot;uri\u0026quot; : \u0026quot;http://localhost:8000/myindex/_search\u0026quot; }, \u0026quot;response\u0026quot; : { \u0026quot;body_length\u0026quot; : 441, \u0026quot;cached\u0026quot; : false, \u0026quot;elapsed\u0026quot; : 9.878, \u0026quot;status_code\u0026quot; : 200, \u0026quot;body\u0026quot; : \u0026quot;\u0026quot;\u0026quot;{\u0026quot;took\u0026quot;:0,\u0026quot;timed_out\u0026quot;:false,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;successful\u0026quot;:1,\u0026quot;skipped\u0026quot;:0,\u0026quot;failed\u0026quot;:0},\u0026quot;hits\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;max_score\u0026quot;:1.0,\u0026quot;hits\u0026quot;:[{\u0026quot;_index\u0026quot;:\u0026quot;myindex\u0026quot;,\u0026quot;_type\u0026quot;:\u0026quot;doc\u0026quot;,\u0026quot;_id\u0026quot;:\u0026quot;c132mhq3r0otidqkac1g\u0026quot;,\u0026quot;_score\u0026quot;:1.0,\u0026quot;_source\u0026quot;:{\u0026quot;name\u0026quot;:\u0026quot;local\u0026quot;,\u0026quot;enabled\u0026quot;:true,\u0026quot;endpoint\u0026quot;:\u0026quot;http://localhost:9200\u0026quot;,\u0026quot;basic_auth\u0026quot;:{},\u0026quot;discovery\u0026quot;:{\u0026quot;refresh\u0026quot;:{}},\u0026quot;created\u0026quot;:\u0026quot;2021-03-08T21:48:55.687557+08:00\u0026quot;,\u0026quot;updated\u0026quot;:\u0026quot;2021-03-08T21:48:55.687557+08:00\u0026quot;}}]}}\u0026quot;\u0026quot;\u0026quot;, \u0026quot;header\u0026quot; : { \u0026quot;UPSTREAM\u0026quot; : \u0026quot;localhost:9200\u0026quot;, \u0026quot;process\u0026quot; : \u0026quot;request_body_regex_replace-\u0026gt;get_cache-\u0026gt;date_range_precision_tuning-\u0026gt;get_cache-\u0026gt;elasticsearch-\u0026gt;set_cache\u0026quot;, \u0026quot;content-length\u0026quot; : \u0026quot;441\u0026quot;, \u0026quot;content-type\u0026quot; : \u0026quot;application/json; charset=UTF-8\u0026quot;, \u0026quot;Server\u0026quot; : \u0026quot;INFINI\u0026quot;, \u0026quot;CLUSTER\u0026quot; : \u0026quot;dev\u0026quot; }, \u0026quot;local_addr\u0026quot; : \u0026quot;127.0.0.1:63310\u0026quot; } } } Parameter Description #     Name Type Description     pattern string Regular expression used for matching and replacement   to string Target string used for replacement    "});index.add({'id':65,'href':'/docs/references/filters/request_client_ip_filter/','title':"request_client_ip_filter",'section':"Online Filter",'content':"request_client_ip_filter #  Description #  The request_client_ip_filter is used to filter traffic based on source user IP addresses of requests.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_client_ip_filter: exclude: - 192.168.3.67 The above example shows that requests from 192.168.3.67 are not allowed to pass through.\nThe following is an example of route redirection.\nflow: - name: echo filter: - echo: message: hello stanger - name: default_flow filter: - request_client_ip_filter: action: redirect_flow flow: echo exclude: - 192.168.3.67 Requests from 192.168.3.67 are redirected to another echo flow.\nParameter Description #     Name Type Description     exclude array List of IP arrays, from which requests are refused to pass through   include array List of IP arrays, from which requests are allowed to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  "});index.add({'id':66,'href':'/docs/references/filters/request_client_ip_limiter/','title':"request_client_ip_limiter",'section':"Online Filter",'content':"request_client_ip_limiter #  Description #  The request_client_ip_limiter filter is used to control traffic based on the request client IP address.\nConfiguration Example #  A configuration example is as follows:\nflow: - name: rate_limit_flow filter: - request_client_ip_limiter: ip: #only limit for specify ips - 127.0.0.1 max_requests: 256 # max_bytes: 102400 #100k action: retry # retry or drop # max_retry_times: 1000 # retry_interval: 500 #100ms message: \u0026quot;your ip reached our limit\u0026quot; The above configuration controls the traffic with the IP address of 127.0.0.1 and the allowable maximum QPS is 256 per second.\nParameter Description #     Name Type Description     ip array Client IP addresses that will participate in traffic control. If this parameter is not set, all IP addresses will participate in traffic control.   interval string Interval for evaluating whether traffic control conditions are met. The default value is 1s.   max_requests int Maximum request count limit in the interval   max_bytes int Maximum request traffic limit in the interval   action string Processing action after traffic control is triggered. The value can be set as retry or drop and the default value is retry.   status string Status code returned after traffic control conditions are met. The default value is 429.   message string Rejection message returned for a request, for which traffic control conditions are met   retry_interval int Interval for traffic control retry, in milliseconds. The default value is 10.   max_retry_times int Maximum retry count in the case of traffic control retries. The default value is 1000.   failed_retry_message string Rejection message returned for a request, for which the maximum retry count has been reached    "});index.add({'id':67,'href':'/docs/references/filters/request_header_filter/','title':"request_header_filter",'section':"Online Filter",'content':"request_header_filter #  Description #  The request_header_filter is used to filter traffic based on request header information.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_header_filter: include: - TRACE: true The above example shows that requests are allowed to pass through only when the headers of the requests contain TRACE: true.\ncurl 192.168.3.4:8000 -v -H 'TRACE: true' Parameter Description #     Name Type Description     exclude array Header information used to refuse to allow requests to pass through   include array Header information used to allow requests to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  "});index.add({'id':68,'href':'/docs/references/filters/request_host_filter/','title':"request_host_filter",'section':"Online Filter",'content':"request_host_filter #  Description #  The request_host_filter is used to filter requests based on a specified domain name or host name. It is suitable for scenarios in which there is only one IP address but access control is required for multiple domain names.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_host_filter: include: - domain-test2.com:8000 The above example shows that only requests that are used to access the domain name domain-test2.com:8000 are allowed to pass through.\nExample #  ✗ curl -k -u medcl:backsoon http://domain-test4.com:8000/ -v * Trying 192.168.3.67... * TCP_NODELAY set * Connected to domain-test4.com (192.168.3.67) port 8000 (#0) * Server auth using Basic with user 'medcl' \u0026gt; GET / HTTP/1.1 \u0026gt; Host: domain-test4.com:8000 \u0026gt; Authorization: Basic bWVkY2w6YmFja3Nvb24= \u0026gt; User-Agent: curl/7.64.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 403 Forbidden \u0026lt; Server: INFINI \u0026lt; Date: Fri, 15 Jan 2021 13:53:01 GMT \u0026lt; Content-Length: 0 \u0026lt; FILTERED: true \u0026lt; * Connection #0 to host domain-test4.com left intact * Closing connection 0 ✗ curl -k -u medcl:backsoon http://domain-test2.com:8000/ -v * Trying 192.168.3.67... * TCP_NODELAY set * Connected to domain-test2.com (192.168.3.67) port 8000 (#0) * Server auth using Basic with user 'medcl' \u0026gt; GET / HTTP/1.1 \u0026gt; Host: domain-test2.com:8000 \u0026gt; Authorization: Basic bWVkY2w6YmFja3Nvb24= \u0026gt; User-Agent: curl/7.64.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: INFINI \u0026lt; Date: Fri, 15 Jan 2021 13:52:53 GMT \u0026lt; Content-Type: application/json; charset=UTF-8 \u0026lt; Content-Length: 480 \u0026lt; UPSTREAM: 192.168.3.203:9200 \u0026lt; CACHE-HASH: a2902f950b4ade804b21a062257387ef \u0026lt; { \u0026quot;name\u0026quot; : \u0026quot;node3\u0026quot;, \u0026quot;cluster_name\u0026quot; : \u0026quot;pi\u0026quot;, \u0026quot;cluster_uuid\u0026quot; : \u0026quot;Z_HcN_6ESKWicV-eLsyU4g\u0026quot;, \u0026quot;version\u0026quot; : { \u0026quot;number\u0026quot; : \u0026quot;6.4.2\u0026quot;, \u0026quot;build_flavor\u0026quot; : \u0026quot;default\u0026quot;, \u0026quot;build_type\u0026quot; : \u0026quot;tar\u0026quot;, \u0026quot;build_hash\u0026quot; : \u0026quot;04711c2\u0026quot;, \u0026quot;build_date\u0026quot; : \u0026quot;2018-09-26T13:34:09.098244Z\u0026quot;, \u0026quot;build_snapshot\u0026quot; : false, \u0026quot;lucene_version\u0026quot; : \u0026quot;7.4.0\u0026quot;, \u0026quot;minimum_wire_compatibility_version\u0026quot; : \u0026quot;5.6.0\u0026quot;, \u0026quot;minimum_index_compatibility_version\u0026quot; : \u0026quot;5.0.0\u0026quot; }, \u0026quot;tagline\u0026quot; : \u0026quot;You Know, for Search\u0026quot; } * Connection #0 to host domain-test2.com left intact * Closing connection 0 Parameter Description #     Name Type Description     exclude array List of hosts, from which requests are refused to pass through   include array List of hosts, from which requests are allowed to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  "});index.add({'id':69,'href':'/docs/references/filters/request_host_limiter/','title':"request_host_limiter",'section':"Online Filter",'content':"request_host_limiter #  Description #  The request_host_limiter filter is used to control traffic based on the request host (domain name).\nConfiguration Example #  A configuration example is as follows:\nflow: - name: rate_limit_flow filter: - request_host_limiter: host: - api.elasticsearch.cn:8000 - logging.elasticsearch.cn:8000 max_requests: 256 # max_bytes: 102400 #100k action: retry # retry or drop # max_retry_times: 1000 # retry_interval: 500 #100ms message: \u0026quot;you reached our limit\u0026quot; The above configuration controls the traffic used for accessing domain names api.elasticsearch.cn and logging.elasticsearch.cn and the maximum allowable QPS is 256 per second.\nParameter Description #     Name Type Description     host array Host domain names that will participate in traffic control. If this parameter is not set, all host domain names will participate in traffic control. If an accessed domain name contains a port ID, add the port ID here. For example, localhost:8080.   interval string Interval for evaluating whether traffic control conditions are met. The default value is 1s.   max_requests int Maximum request count limit in the interval   max_bytes int Maximum request traffic limit in the interval   action string Processing action after traffic control is triggered. The value can be set to retry or drop and the default value is retry.   status string Status code returned after traffic control conditions are met. The default value is 429.   message string Rejection message returned for a request, for which traffic control conditions are met   retry_interval int Interval for traffic control retry, in milliseconds. The default value is 10.   max_retry_times int Maximum retry count in the case of traffic control retries. The default value is 1000.   failed_retry_message string Rejection message returned for a request, for which the maximum retry count has been reached    "});index.add({'id':70,'href':'/docs/references/filters/request_method_filter/','title':"request_method_filter",'section':"Online Filter",'content':"request_method_filter #  Description #  The request_method_filter is used to filter traffic based on request method.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_method_filter: exclude: - PUT - POST include: - GET - HEAD - DELETE Parameter Description #     Name Type Description     exclude array Methods of requests that are refused to pass through   include array Methods of requests that are allowed to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  "});index.add({'id':71,'href':'/docs/references/filters/request_path_filter/','title':"request_path_filter",'section':"Online Filter",'content':"request_path_filter #  Description #  The request_path_filter is used to filter traffic based on request path.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_path_filter: must: #must match all rules to continue prefix: - /medcl contain: - _search suffix: - _count - _refresh wildcard: - /*/_refresh regex: - ^/m[\\w]+dcl must_not: # any match will be filtered prefix: - /.kibana - /_security - /_security - /gateway_requests* - /.reporting - /_monitoring/bulk contain: - _search suffix: - _count - _refresh wildcard: - /*/_refresh regex: - ^/m[\\w]+dcl should: prefix: - /medcl contain: - _search - _async_search suffix: - _refresh wildcard: - /*/_refresh regex: - ^/m[\\w]+dcl Parameter Description #     Name Type Description     must.* object Requests are allowed to pass through only when all conditions are met.   must_not.* object Requests are allowed to pass through only when none of the conditions are met.   should.* object Requests are allowed to pass through when any condition is met.   *.prefix array Whether a request begins with a specific character   *.suffix array Whether a request ends with a specific character   *.contain array Whether a request contains a specific character   *.wildcard array Whether a request meets pattern matching rules   *.regex array Whether a request meets regular expression matching rules   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If only the should condition is met, requests are allowed to pass through only when at least one item in should is met.\n"});index.add({'id':72,'href':'/docs/references/filters/request_path_limiter/','title':"request_path_limiter",'section':"Online Filter",'content':"request_path_limiter #  Description #  The request_path_limiter filter is used to define traffic control rules for requests. It can implement index-level traffic control.\nConfiguration Example #  A configuration example is as follows:\nflow: - name: rate_limit_flow filter: - request_path_limiter: message: \u0026quot;Hey, You just reached our request limit!\u0026quot; rules: - pattern: \u0026quot;/(?P\u0026lt;index_name\u0026gt;medcl)/_search\u0026quot; max_qps: 3 group: index_name - pattern: \u0026quot;/(?P\u0026lt;index_name\u0026gt;.*?)/_search\u0026quot; max_qps: 100 group: index_name In the above configuration, the query is performed against the medcl query, the allowable maximum QPS is 3, and the QPS is 100 for queries performed against other indexes.\nParameter Description #     Name Type Description     message string Message returned for a request, for which traffic control conditions are met   rules array Traffic control rule. Multiple rules can be configured, which are matched based on their configuration sequence. If a rule is matched earlier, the corresponding action is performed earlier.   rules.pattern string Regular expression rule used for URL path matching. One group name must be provided as the bucket key for traffic control.   rules.group string Group name defined in the regular expression, which is used to count the number of requests. Requests with the same group value are regarded as the same type of request.   rules.max_qps int Maximum QPS defined for each group of requests. When the actual value exceeds this value, the traffic control action is triggered.    "});index.add({'id':73,'href':'/docs/references/filters/request_user_filter/','title':"request_user_filter",'section':"Online Filter",'content':"request_user_filter #  Description #  When Elasticsearch conducts authentication in Basic Auth mode, the request_user_filter is used to filter requests by request username.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_user_filter: include: - \u0026quot;elastic\u0026quot; The above example shows that only requests from elastic are allowed to pass through.\nParameter Description #     Name Type Description     exclude array List of usernames, from which requests are refused to pass through   include array List of usernames, from which requests are allowed to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  "});index.add({'id':74,'href':'/docs/references/filters/request_user_limiter/','title':"request_user_limiter",'section':"Online Filter",'content':"request_user_limiter #  Description #  The request_user_limiter filter is used to control traffic by username.\nConfiguration Example #  A configuration example is as follows:\nflow: - name: rate_limit_flow filter: - request_user_limiter: user: - elastic - medcl max_requests: 256 # max_bytes: 102400 #100k action: retry # retry or drop # max_retry_times: 1000 # retry_interval: 500 #100ms message: \u0026quot;you reached our limit\u0026quot; The above configuration controls the traffic of users medcl and elastic and the allowable maximum QPS is 256 per second.\nParameter Description #     Name Type Description     user array Users who will participate in traffic control. If this parameter is not set, all users will participate in traffic control.   interval string Interval for evaluating whether traffic control conditions are met. The default value is 1s.   max_requests int Maximum request count limit in the interval   max_bytes int Maximum request traffic limit in the interval   action string Processing action after traffic control is triggered. The value can be set as retry or drop and the default value is retry.   message string Rejection message returned for a request, for which traffic control conditions are met   retry_interval int Interval for traffic control retry, in milliseconds. The default value is 10.   max_retry_times int Maximum retry count in the case of traffic control retries. The default value is 1000.    "});index.add({'id':75,'href':'/docs/references/filters/response_body_regex_replace/','title':"response_body_regex_replace",'section':"Online Filter",'content':"response_body_regex_replace #  Description #  The response_body_regex_replace filter is used to replace string content in a response by using a regular expression.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - echo: message: \u0026quot;hello infini\\n\u0026quot; - response_body_regex_replace: pattern: infini to: world The result output of the preceding example is hello world.\nParameter Description #     Name Type Description     pattern string Regular expression used for matching and replacement   to string Target string used for replacement    "});index.add({'id':76,'href':'/docs/references/filters/response_header_filter/','title':"response_header_filter",'section':"Online Filter",'content':"response_header_filter #  Description #  The response_header_filter is used to filter traffic based on response header information.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: ... - response_header_filter: exclude: - INFINI-CACHE: CACHED The above example shows that a request is not allowed to pass through when the header information of the response contains INFINI-CACHE: CACHED.\nParameter Description #     Name Type Description     exclude array Response header information for refusing to allow traffic to pass through   include array Response header information for allowing traffic to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  "});index.add({'id':77,'href':'/docs/references/filters/response_header_format/','title':"response_header_format",'section':"Online Filter",'content':"response_header_format #  Description #  The response_header_format filter is used to convert keys in response header information into lowercase letters.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - response_header_format: "});index.add({'id':78,'href':'/docs/references/filters/response_status_filter/','title':"response_status_filter",'section':"Online Filter",'content':"response_status_filter #  Description #  The response_status_filter is used to filter traffic based on the status code responded by the back-end service.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - response_status_filter: message: \u0026quot;Request filtered!\u0026quot; exclude: - 404 include: - 200 - 201 - 500 Parameter Description #     Name Type Description     exclude array Response code for refusing to allow traffic to pass through   include array Response code for allowing traffic to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  "});index.add({'id':79,'href':'/docs/references/filters/retry_limiter/','title':"retry_limiter",'section':"Online Filter",'content':"retry_limiter #  Description #  The retry_limiter filter is used to judge whether the maximum retry count is reached for a request, to avert unlimited retries of a request.\nConfiguration Example #  A simple example is as follows:\nflow: - name: retry_limiter filter: - retry_limiter: queue_name: \u0026quot;deadlock_messages\u0026quot; max_retry_times: 3 Parameter Description #     Name Type Description     max_retry_times int Maximum retry count. The default value is 3.   queue_name string Name of a message queue, to which messages are output after the maximum retry count is reached   tag_on_success array Specified tag to be attached to request context after retry conditions are triggered    "});index.add({'id':80,'href':'/docs/references/filters/sample/','title':"sample",'section':"Online Filter",'content':"sample #  Description #  The sample filter is used to sample normal traffic proportionally. In a massive query scenario, collecting logs of all traffic consumes considerable resources. Therefore, you are advised to perform sampling statistics and sample and analyze query logs.\nConfiguration Example #  A simple example is as follows:\nflow: - name: sample filter: - sample: ratio: 0.2 Parameter Description #     Name Type Description     ratio float Sampling ratio    "});index.add({'id':81,'href':'/docs/references/filters/set_basic_auth/','title':"set_basic_auth",'section':"Online Filter",'content':"set_basic_auth #  Description #  The set_basic_auth filter is used to configure the authentication information used for requests. You can use the filter to reset the authentication information used for requests.\nConfiguration Example #  A simple example is as follows:\nflow: - name: set_basic_auth filter: - set_basic_auth: username: admin password: password Parameter Description #     Name Type Description     username string Username   password string Password    "});index.add({'id':82,'href':'/docs/references/filters/set_context/','title':"set_context",'section':"Online Filter",'content':"set_context #  Description #  The set_context filter is used to set relevant information for the request context.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - set_response: body: '{\u0026quot;message\u0026quot;:\u0026quot;hello world\u0026quot;}' - set_context: context: # _ctx.request.uri: http://baidu.com # _ctx.request.path: new_request_path # _ctx.request.host: api.infinilabs.com # _ctx.request.method: DELETE # _ctx.request.body: \u0026quot;hello world\u0026quot; # _ctx.request.body_json.explain: true # _ctx.request.query_args.from: 100 # _ctx.request.header.ENV: dev # _ctx.response.content_type: \u0026quot;application/json\u0026quot; # _ctx.response.header.TIMES: 100 # _ctx.response.status: 419 # _ctx.response.body: \u0026quot;new_body\u0026quot; _ctx.response.body_json.success: true - dump: request: true Parameter Description #     Name Type Description     context map Request context and corresponding value    A list of supported context variables is provided below:\n   Name Type Description     _ctx.request.uri string Complete URL of a request   _ctx.request.path string Request path   _ctx.request.host string Request host   _ctx.request.method string Request method type   _ctx.request.body string Request body   _ctx.request.body_json.[JSON_PATH] string Path to the JSON request object   _ctx.request.query_args.[KEY] string URL query request parameter   _ctx.request.header.[KEY] string Request header information   _ctx.response.content_type string Request body type   _ctx.response.header.[KEY] string Response header information   _ctx.response.status int Returned status code   _ctx.response.body string Returned response body   _ctx.response.body_json.[JSON_PATH] string Path to the JSON response object    "});index.add({'id':83,'href':'/docs/references/filters/set_hostname/','title':"set_hostname",'section':"Online Filter",'content':"set_hostname #  Description #  The set_hostname filter is used to set the host or domain name to be accessed in the request header.\nConfiguration Example #  A simple example is as follows:\nflow: - name: set_hostname filter: - set_hostname: hostname: api.infini.sh Parameter Description #     Name Type Description     hostname string Host information    "});index.add({'id':84,'href':'/docs/references/filters/set_request_header/','title':"set_request_header",'section':"Online Filter",'content':"set_request_header #  Description #  The set_request_header filter is used to set header information for requests.\nConfiguration Example #  A simple example is as follows:\nflow: - name: set_request_header filter: - set_request_header: headers: - Trial -\u0026gt; true - Department -\u0026gt; Engineering Parameter Description #     Name Type Description     headers map It uses -\u0026gt; to identify a key value pair and set header information.    "});index.add({'id':85,'href':'/docs/references/filters/set_request_query_args/','title':"set_request_query_args",'section':"Online Filter",'content':"set_request_query_args #  Description #  The set_request_query_args filter is used to set the QueryString parameter information used for requests.\nConfiguration Example #  A simple example is as follows:\nflow: - name: set_request_query_args filter: - set_request_query_args: args: - size -\u0026gt; 10 Parameter Description #     Name Type Description     args map It uses -\u0026gt; to identify a key value pair and set QueryString parameter information.    "});index.add({'id':86,'href':'/docs/references/filters/set_response/','title':"set_response",'section':"Online Filter",'content':"set_response #  Description #  The set_response filter is used to set response information to be returned for requests.\nConfiguration Example #  A simple example is as follows:\nflow: - name: set_response filter: - set_response: status: 200 content_type: application/json body: '{\u0026quot;message\u0026quot;:\u0026quot;hello world\u0026quot;}' Parameter Description #     Name Type Description     status int Request status code, which is 200 by default.   content_type string Type of returned content   body string Returned structural body    "});index.add({'id':87,'href':'/docs/references/filters/set_response_header/','title':"set_response_header",'section':"Online Filter",'content':"set_response_header #  Description #  The set_response_header filter is used to set the header information used in responses.\nConfiguration Example #  A simple example is as follows:\nflow: - name: set_response_header filter: - set_response_header: headers: - Trial -\u0026gt; true - Department -\u0026gt; Engineering Parameter Description #     Name Type Description     headers map It uses -\u0026gt; to identify a key value pair and set header information.    "});index.add({'id':88,'href':'/docs/references/filters/sleep/','title':"sleep",'section':"Online Filter",'content':"sleep #  Description #  The sleep filter is used to add a fixed delay to requests to reduce the speed.\nConfiguration Example #  A simple example is as follows:\nflow: - name: slow_query_logging_test filter: - sleep: sleep_in_million_seconds: 1024 Parameter Description #     Name Type Description     sleep_in_million_seconds int64 Delay to be added, in milliseconds    "});index.add({'id':89,'href':'/docs/references/filters/switch/','title':"switch",'section':"Online Filter",'content':"switch #  Description #  The switch filter is used to forward traffic to another flow along the requested path, to facilitate cross-cluster operations. No alternation is required for Elasticsearch clusters, and all APIs in each cluster can be accessed, including APIs used for index read/write and cluster operations. \\\nConfiguration Example #  A simple example is as follows:\nflow: - name: es1-flow filter: - elasticsearch: elasticsearch: es1 - name: es2-flow filter: - elasticsearch: elasticsearch: es2 - name: cross_cluste_search filter: - switch: path_rules: - prefix: \u0026quot;es1:\u0026quot; flow: es1-flow - prefix: \u0026quot;es2:\u0026quot; flow: es2-flow - elasticsearch: elasticsearch: dev #elasticsearch configure reference name In the above example, the index beginning with es1: is forwarded to the es1 cluster, the index beginning with es2: is forwarded to the es2 cluster, and unmatched indexes are forwarded to the dev cluster. Clusters of different versions can be controlled within one Kibana. See the following example.\n# GET es1:_cluster/health { \u0026quot;cluster_name\u0026quot; : \u0026quot;elasticsearch\u0026quot;, \u0026quot;status\u0026quot; : \u0026quot;yellow\u0026quot;, \u0026quot;timed_out\u0026quot; : false, \u0026quot;number_of_nodes\u0026quot; : 1, \u0026quot;number_of_data_nodes\u0026quot; : 1, \u0026quot;active_primary_shards\u0026quot; : 37, \u0026quot;active_shards\u0026quot; : 37, \u0026quot;relocating_shards\u0026quot; : 0, \u0026quot;initializing_shards\u0026quot; : 0, \u0026quot;unassigned_shards\u0026quot; : 9, \u0026quot;delayed_unassigned_shards\u0026quot; : 0, \u0026quot;number_of_pending_tasks\u0026quot; : 0, \u0026quot;number_of_in_flight_fetch\u0026quot; : 0, \u0026quot;task_max_waiting_in_queue_millis\u0026quot; : 0, \u0026quot;active_shards_percent_as_number\u0026quot; : 80.43478260869566 } # GET es2:_cluster/health { \u0026quot;cluster_name\u0026quot; : \u0026quot;elasticsearch\u0026quot;, \u0026quot;status\u0026quot; : \u0026quot;yellow\u0026quot;, \u0026quot;timed_out\u0026quot; : false, \u0026quot;number_of_nodes\u0026quot; : 1, \u0026quot;number_of_data_nodes\u0026quot; : 1, \u0026quot;active_primary_shards\u0026quot; : 6, \u0026quot;active_shards\u0026quot; : 6, \u0026quot;relocating_shards\u0026quot; : 0, \u0026quot;initializing_shards\u0026quot; : 0, \u0026quot;unassigned_shards\u0026quot; : 6, \u0026quot;delayed_unassigned_shards\u0026quot; : 0, \u0026quot;number_of_pending_tasks\u0026quot; : 0, \u0026quot;number_of_in_flight_fetch\u0026quot; : 0, \u0026quot;task_max_waiting_in_queue_millis\u0026quot; : 0, \u0026quot;active_shards_percent_as_number\u0026quot; : 50.0 } You can run commands to achieve the same effect.\nroot@infini:/opt/gateway# curl -v 192.168.3.4:8000/es1:_cat/nodes * Trying 192.168.3.4... * TCP_NODELAY set * Connected to 192.168.3.4 (192.168.3.4) port 8000 (#0) \u0026gt; GET /es1:_cat/nodes HTTP/1.1 \u0026gt; Host: 192.168.3.4:8000 \u0026gt; User-Agent: curl/7.58.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: INFINI \u0026lt; Date: Thu, 14 Oct 2021 10:37:39 GMT \u0026lt; content-type: text/plain; charset=UTF-8 \u0026lt; Content-Length: 45 \u0026lt; X-Backend-Cluster: dev1 \u0026lt; X-Backend-Server: 192.168.3.188:9299 \u0026lt; X-Filters: filters-\u0026gt;switch-\u0026gt;filters-\u0026gt;elasticsearch-\u0026gt;skipped \u0026lt; 192.168.3.188 48 38 5 cdhilmrstw * LENOVO * Connection #0 to host 192.168.3.4 left intact root@infini:/opt/gateway# curl -v 192.168.3.4:8000/es2:_cat/nodes * Trying 192.168.3.4... * TCP_NODELAY set * Connected to 192.168.3.4 (192.168.3.4) port 8000 (#0) \u0026gt; GET /es2:_cat/nodes HTTP/1.1 \u0026gt; Host: 192.168.3.4:8000 \u0026gt; User-Agent: curl/7.58.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: INFINI \u0026lt; Date: Thu, 14 Oct 2021 10:37:48 GMT \u0026lt; content-type: text/plain; charset=UTF-8 \u0026lt; Content-Length: 146 \u0026lt; X-elastic-product: Elasticsearch \u0026lt; Warning: 299 Elasticsearch-7.14.0-dd5a0a2acaa2045ff9624f3729fc8a6f40835aa1 \u0026quot;Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.14/security-minimal-setup.html to enable security.\u0026quot; \u0026lt; X-Backend-Cluster: dev \u0026lt; X-Backend-Server: 192.168.3.188:9216 \u0026lt; X-Filters: filters-\u0026gt;switch-\u0026gt;filters-\u0026gt;elasticsearch-\u0026gt;skipped \u0026lt; 192.168.3.188 26 38 3 cdfhilmrstw - node-714-1 192.168.3.188 45 38 3 cdfhilmrstw * LENOVO 192.168.3.188 43 38 4 cdfhilmrstw - node-714-2 * Connection #0 to host 192.168.3.4 left intact Parameter Description #     Name Type Description     path_rules array Matching rule based on the URL   path_rules.prefix string Prefix string for matching. It is recommended that the prefix string end with :. After matching, the URL prefix is removed from the traffic, which is then forwarded to the subsequent flow.   path_rules.flow string Name of the flow for processing a matched request   remove_prefix bool Whether to remove matched prefix string before request forwarding. The default value is true.    "});index.add({'id':90,'href':'/docs/references/filters/translog/','title':"translog",'section':"Online Filter",'content':"translog #  Description #  The translog filter is used to save received requests to local files and compress them. It can record some or complete request logs for archiving and request replay.\nConfiguration Example #  A simple example is as follows:\nflow: - name: translog filter: - translog: max_file_age: 7 max_file_count: 10 Parameter Description #     Name Type Description     path string Root directory for log storage, which is the translog subdirectory in the gateway data directory by default   category string Level-2 subdirectory for differentiating different logs, which is default by default.   filename string Name of the log storage file, which is translog.log by default.   compress bool Whether to compress and archive files after scrolling. The default value is true.   max_file_age int Maximum number of days that archived files can be retained, which is 30 days by default.   max_file_count int Maximum number of archived files that can be retained, which is 100 by default.   max_file_size_in_mb int Maximum size of a single archived file, in bytes. The default value is 1024 MB.    "});})();